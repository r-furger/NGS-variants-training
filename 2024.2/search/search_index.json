{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Teachers Geert van Geest .st0{fill:#A6CE39;} .st1{fill:#FFFFFF;} Authors Geert van Geest .st0{fill:#A6CE39;} .st1{fill:#FFFFFF;} Patricia Palagi .st0{fill:#A6CE39;} .st1{fill:#FFFFFF;} License & copyright License: CC BY-SA 4.0 Copyright: SIB Swiss Institute of Bioinformatics Learning outcomes After this course, you will be able to: Understand important aspects of NGS and read alignment for variant analysis Perform a read alignment ready for variant analysis Perform variant calling according to GATK best practices Perform a variant annotation Learning experiences This course will consist of lectures, exercises and polls. During exercises, you are free to discuss with other participants. During lectures, focus on the lecture only.","title":"Home"},{"location":"#teachers","text":"Geert van Geest .st0{fill:#A6CE39;} .st1{fill:#FFFFFF;}","title":"Teachers"},{"location":"#authors","text":"Geert van Geest .st0{fill:#A6CE39;} .st1{fill:#FFFFFF;} Patricia Palagi .st0{fill:#A6CE39;} .st1{fill:#FFFFFF;}","title":"Authors"},{"location":"#license-copyright","text":"License: CC BY-SA 4.0 Copyright: SIB Swiss Institute of Bioinformatics","title":"License &amp; copyright"},{"location":"#learning-outcomes","text":"After this course, you will be able to: Understand important aspects of NGS and read alignment for variant analysis Perform a read alignment ready for variant analysis Perform variant calling according to GATK best practices Perform a variant annotation","title":"Learning outcomes"},{"location":"#learning-experiences","text":"This course will consist of lectures, exercises and polls. During exercises, you are free to discuss with other participants. During lectures, focus on the lecture only.","title":"Learning experiences"},{"location":"course_schedule/","text":"Note Apart from the starting time the time schedule is indicative . Because we can not plan a course by the minute, in practice the time points will deviate. Day 1 block start end subject block 1 9:15 AM 10:30 AM Introduction 10:30 AM 11:00 AM BREAK block 2 11:00 AM 12:30 PM Setup & Reproducibility 12:30 PM 1:30 PM BREAK block 3 1:30 PM 3:00 PM Read alignment - basics 3:00 PM 3:30 PM BREAK block 4 3:30 PM 5:00 PM Read alignment - advanced Day 2 block start end subject block 1 9:15 AM 10:30 AM Variant calling - preparation 10:30 AM 11:00 AM BREAK block 2 11:00 AM 12:30 PM Variant calling 12:30 PM 1:30 PM BREAK block 3 1:30 PM 3:00 PM Filtering & evaluation & Visualisation 3:00 PM 3:30 PM BREAK block 4 3:30 PM 5:00 PM Annotation","title":"Course schedule"},{"location":"course_schedule/#day-1","text":"block start end subject block 1 9:15 AM 10:30 AM Introduction 10:30 AM 11:00 AM BREAK block 2 11:00 AM 12:30 PM Setup & Reproducibility 12:30 PM 1:30 PM BREAK block 3 1:30 PM 3:00 PM Read alignment - basics 3:00 PM 3:30 PM BREAK block 4 3:30 PM 5:00 PM Read alignment - advanced","title":"Day 1"},{"location":"course_schedule/#day-2","text":"block start end subject block 1 9:15 AM 10:30 AM Variant calling - preparation 10:30 AM 11:00 AM BREAK block 2 11:00 AM 12:30 PM Variant calling 12:30 PM 1:30 PM BREAK block 3 1:30 PM 3:00 PM Filtering & evaluation & Visualisation 3:00 PM 3:30 PM BREAK block 4 3:30 PM 5:00 PM Annotation","title":"Day 2"},{"location":"precourse/","text":"UNIX As is stated in the course prerequisites at the announcement web page , we expect participants to have a basic understanding of working with the command line on UNIX-based systems. You can test your UNIX skills with a quiz here . If you don\u2019t have experience with UNIX command line, or if you\u2019re unsure whether you meet the prerequisites, follow our online UNIX tutorial . Software We will be mainly working on an Amazon Web Services ( AWS ) Elastic Cloud (EC2) server. Our Ubuntu server behaves like a \u2018normal\u2019 remote server, and can be approached through a VS code web interface. All participants will be granted access to a personal workspace to be used during the course. The only software you need to install before the course is Integrative Genomics Viewer (IGV) . For self-learners Self learners can install the required software via conda, or run the VS code web interface inside a container. More instructions at Setup","title":"Precourse preparations"},{"location":"precourse/#unix","text":"As is stated in the course prerequisites at the announcement web page , we expect participants to have a basic understanding of working with the command line on UNIX-based systems. You can test your UNIX skills with a quiz here . If you don\u2019t have experience with UNIX command line, or if you\u2019re unsure whether you meet the prerequisites, follow our online UNIX tutorial .","title":"UNIX"},{"location":"precourse/#software","text":"We will be mainly working on an Amazon Web Services ( AWS ) Elastic Cloud (EC2) server. Our Ubuntu server behaves like a \u2018normal\u2019 remote server, and can be approached through a VS code web interface. All participants will be granted access to a personal workspace to be used during the course. The only software you need to install before the course is Integrative Genomics Viewer (IGV) . For self-learners Self learners can install the required software via conda, or run the VS code web interface inside a container. More instructions at Setup","title":"Software"},{"location":"day1/alignment/","text":"Learning outcomes After having completed this chapter you will be able to: Describe the general workflow of library preparation and sequencing with an Illumina sequencer Explain how the fastq format stores sequence and base quality information Calculate probability from phred quality and the other way around Explain why base quality and mapping quality are important for detecting variants Illustrate the difference between short-read and long-read sequencing Explain which type of invention led to development of long-read sequencing Explain what impact long read sequencing can have on variant analysis Describe how alignment information is stored in a sequence alignment ( .sam ) file Define a duplicate alignment and explain how alignment duplicates can affect variant analysis Perform an alignment of genomic reads with bwa mem Generate and interpret the alignment statistics from samtools flagstat Material Download the presentation Exercises Running into problems during the exercises? Use the \u201cComments\u201d box at the bottom of each page \ud83d\udc47 for asking questions or giving feedback. It requires a github account . 1. Download data and prepare the reference genome Let\u2019s start with the first script of our \u2018pipeline\u2019. We will use it to download and unpack the course data. Use the code snippet below to create a script called A01_download_course_data.sh . Store it in ~/project/scripts/A-prepare_references/ , and run it. A01_download_course_data.sh #!/usr/bin/env bash cd ~/project wget https://ngs-variants-training.s3.eu-central-1.amazonaws.com/ngs-variants-training.tar.gz tar -xvf ngs-variants-training.tar.gz rm ngs-variants-training.tar.gz Exercise: This will create the directory data . Check out what\u2019s in there. Answer The directory data contains the following: data \u251c\u2500\u2500 fastq \u2502 \u251c\u2500\u2500 father_R1.fastq.gz \u2502 \u251c\u2500\u2500 father_R2.fastq.gz \u2502 \u251c\u2500\u2500 mother_R1.fastq.gz \u2502 \u251c\u2500\u2500 mother_R2.fastq.gz \u2502 \u251c\u2500\u2500 son_R1.fastq.gz \u2502 \u2514\u2500\u2500 son_R2.fastq.gz \u251c\u2500\u2500 reference \u2502 \u2514\u2500\u2500 Homo_sapiens.GRCh38.dna.chromosome.20.fa \u2514\u2500\u2500 variants \u251c\u2500\u2500 1000g_gold_standard.indels.filtered.vcf \u251c\u2500\u2500 GCF.38.filtered.renamed.vcf \u251c\u2500\u2500 NA12878.vcf.gz \u2514\u2500\u2500 NA12878.vcf.gz.tbi 3 directories, 11 files These are: input reads (at fastq ) a part of the human reference genome (at reference ) some vcfs with variants for calibration and evaluation (at variants ) Use data only for input The directory data that you have just downloaded, contains only input files for the exercises. So, don\u2019t write output (except for indexes) to this directory. In order to index the reference sequence we are going to need some bioinformatics tools. All required tools are pre-installed in a conda environment called ngs-tools . In order to use them, every time you open a new terminal, you will have to load the environment: conda activate ngs-tools activate ngs-tools Every time you open a new terminal you will have to activate the environment again. The software bwa is in this environment. We will use it for the alignment. Like all alignment software, it requires an index of the reference genome. You can make an index like this: bwa index <reference.fa> Make an index of the reference sequence of chromosome 20 of the human genome. You can find the fasta file in ~/project/data/reference/Homo_sapiens.GRCh38.dna.chromosome.20.fa . Do it with a script called A02_create_bwa_index.sh . Also store this in the directory A-prepare_references . Answer A02_create_bwa_index.sh #!/usr/bin/env bash cd ~/project/data/reference/ bwa index Homo_sapiens.GRCh38.dna.chromosome.20.fa 2. Read alignment Check out the synopsis and manual of bwa mem . We\u2019ll be using paired-end reads of three samples that can be found at ~/project/data/fastq . If we run bwa mem with default options, which three arguments do we need? Answer The manual says: bwa mem [-aCHMpP] [-t nThreads] [-k minSeedLen] ... db.prefix reads.fq [mates.fq] So, we\u2019ll need: a database prefix ( db.prefix ) forward reads ( reads.fq ) reverse reads ( mates.fq ) For our reference sequence a command would look like: cd ~/project/ bwa mem \\ data/reference/Homo_sapiens.GRCh38.dna.chromosome.20.fa \\ <forward_reads.fq> \\ <reverse_reads.fq> \\ > <alignment.sam> We will now go through all the steps concerning alignment for the sample mother . To store the results of these steps, we will create a directory within ~/project called results . For the alignment, make a script called B01_alignment.sh . Since we will perform a similar analysis later on for all samples, we store this script in ~/project/scripts/B-mother_only . Your directory ~/project/scripts should now like this: scripts \u251c\u2500\u2500 A-prepare_references \u2502 \u251c\u2500\u2500 A01_download_course_data.sh \u2502 \u251c\u2500\u2500 A02_create_bwa_index.sh \u251c\u2500\u2500 B-mother_only \u2502 \u2514\u2500\u2500 B01_alignment.sh \u2514\u2500\u2500 C-all_samples In B01_alignment.sh write the commands to perform an alignment with bwa mem of the reads from the mother ( mother_R1.fastq and mother_R2.fastq ) against chromosome 20. Write the resulting .sam file to a directory in ~/project/results called alignments . Index prefix is the same a reference filename With default values, the name of the index of a reference for bwa mem is the same as the name of the reference itself. In this case, this would be Homo_sapiens.GRCh38.dna.chromosome.20.fa . Answer B01_read_alignment.sh #!/usr/bin/env bash cd ~/project/ mkdir -p results/alignments bwa mem \\ data/reference/Homo_sapiens.GRCh38.dna.chromosome.20.fa \\ data/fastq/mother_R1.fastq.gz \\ data/fastq/mother_R2.fastq.gz \\ > results/alignments/mother.sam 3. Alignment statistics Exercise: Check out the statistics of the alignment by using samtools flagstat . Write the output of samtools flagstat to a file called mother.sam.flagstat . Do this by creating a script called B02_get_alignment_statistics.sh , and add this script to ~/project/scripts/B-mother_only . Find the documentation of samtools flagstat here . Any duplicates in there? Answer B02_get_alignment_statistics.sh #!/usr/bin/env bash cd ~/project/results/alignments samtools flagstat mother.sam > mother.sam.flagstat This should result in: 133477 + 0 in total (QC-passed reads + QC-failed reads) 0 + 0 secondary 317 + 0 supplementary 0 + 0 duplicates 132892 + 0 mapped (99.56% : N/A) 133160 + 0 paired in sequencing 66580 + 0 read1 66580 + 0 read2 131470 + 0 properly paired (98.73% : N/A) 131990 + 0 with itself and mate mapped 585 + 0 singletons (0.44% : N/A) 0 + 0 with mate mapped to a different chr 0 + 0 with mate mapped to a different chr (mapQ>=5) No duplicates were found ( 0 + 0 duplicates ). The aligner doesn\u2019t automatically flag duplicates. This needs to be done after the alignment. 4. Sorting and compression Many downstream analyses require a coordinate sorted alignment file. Now, your alignment file is in the same order as the fastq file. You can coordinate sort an alignment file with samtools sort . You can find the documentation here . Exercise : Sort the alignment file according to coordinate. In order to do this, create a script called B03_sort_alignment.sh (in ~/project/scripts/B-mother_only ). Answer B03_sort_alignment.sh #!/usr/bin/env bash cd ~/project/results samtools sort -o alignments/mother.sorted.sam alignments/mother.sam Tip: samtools sort and samtools view can write to stdout Like bwa mem , samtools sort and samtools view can write its output to stdout. This means that you need to redirect your output to a file with > or use the the output option -o . The command samtools view is very versatile. It takes an alignment file and writes a filtered or processed alignment to the output. You can for example use it to compress your SAM file into a BAM file. Let\u2019s start with that. Exercise : compress our SAM file into a BAM file and include the header in the output. For this, use the -b and -h options. Perform the calculation from a script called B04_compress_alignment.sh (in ~/project/scripts/B-mother_only ). Find the required documentation here . How much was the disk space reduced by compressing the file? Answer B04_compress_alignment.sh #!/usr/bin/env bash cd ~/project/results samtools view -bh alignments/mother.sorted.sam > alignments/mother.bam By using ls -lh , you can find out that mother.sorted.sam has a size of 55 Mb, while mother.bam is only 16 Mb. 5. Recap You have now performed: alignment sorting compression flag statistics On the sample mother . Your scripts directory should look like this: scripts \u251c\u2500\u2500 A-prepare_references \u2502 \u251c\u2500\u2500 A01_download_course_data.sh \u2502 \u2514\u2500\u2500 A02_create_bwa_index.sh \u251c\u2500\u2500 B-mother_only \u2502 \u251c\u2500\u2500 B01_alignment.sh \u2502 \u251c\u2500\u2500 B02_get_alignment_statistics.sh \u2502 \u251c\u2500\u2500 B03_sort_alignment.sh \u2502 \u2514\u2500\u2500 B04_compress_alignment.sh \u2514\u2500\u2500 C-all_samples","title":"Read alignment - basics"},{"location":"day1/alignment/#learning-outcomes","text":"After having completed this chapter you will be able to: Describe the general workflow of library preparation and sequencing with an Illumina sequencer Explain how the fastq format stores sequence and base quality information Calculate probability from phred quality and the other way around Explain why base quality and mapping quality are important for detecting variants Illustrate the difference between short-read and long-read sequencing Explain which type of invention led to development of long-read sequencing Explain what impact long read sequencing can have on variant analysis Describe how alignment information is stored in a sequence alignment ( .sam ) file Define a duplicate alignment and explain how alignment duplicates can affect variant analysis Perform an alignment of genomic reads with bwa mem Generate and interpret the alignment statistics from samtools flagstat","title":"Learning outcomes"},{"location":"day1/alignment/#material","text":"Download the presentation","title":"Material"},{"location":"day1/alignment/#exercises","text":"Running into problems during the exercises? Use the \u201cComments\u201d box at the bottom of each page \ud83d\udc47 for asking questions or giving feedback. It requires a github account .","title":"Exercises"},{"location":"day1/alignment/#1-download-data-and-prepare-the-reference-genome","text":"Let\u2019s start with the first script of our \u2018pipeline\u2019. We will use it to download and unpack the course data. Use the code snippet below to create a script called A01_download_course_data.sh . Store it in ~/project/scripts/A-prepare_references/ , and run it. A01_download_course_data.sh #!/usr/bin/env bash cd ~/project wget https://ngs-variants-training.s3.eu-central-1.amazonaws.com/ngs-variants-training.tar.gz tar -xvf ngs-variants-training.tar.gz rm ngs-variants-training.tar.gz Exercise: This will create the directory data . Check out what\u2019s in there. Answer The directory data contains the following: data \u251c\u2500\u2500 fastq \u2502 \u251c\u2500\u2500 father_R1.fastq.gz \u2502 \u251c\u2500\u2500 father_R2.fastq.gz \u2502 \u251c\u2500\u2500 mother_R1.fastq.gz \u2502 \u251c\u2500\u2500 mother_R2.fastq.gz \u2502 \u251c\u2500\u2500 son_R1.fastq.gz \u2502 \u2514\u2500\u2500 son_R2.fastq.gz \u251c\u2500\u2500 reference \u2502 \u2514\u2500\u2500 Homo_sapiens.GRCh38.dna.chromosome.20.fa \u2514\u2500\u2500 variants \u251c\u2500\u2500 1000g_gold_standard.indels.filtered.vcf \u251c\u2500\u2500 GCF.38.filtered.renamed.vcf \u251c\u2500\u2500 NA12878.vcf.gz \u2514\u2500\u2500 NA12878.vcf.gz.tbi 3 directories, 11 files These are: input reads (at fastq ) a part of the human reference genome (at reference ) some vcfs with variants for calibration and evaluation (at variants ) Use data only for input The directory data that you have just downloaded, contains only input files for the exercises. So, don\u2019t write output (except for indexes) to this directory. In order to index the reference sequence we are going to need some bioinformatics tools. All required tools are pre-installed in a conda environment called ngs-tools . In order to use them, every time you open a new terminal, you will have to load the environment: conda activate ngs-tools activate ngs-tools Every time you open a new terminal you will have to activate the environment again. The software bwa is in this environment. We will use it for the alignment. Like all alignment software, it requires an index of the reference genome. You can make an index like this: bwa index <reference.fa> Make an index of the reference sequence of chromosome 20 of the human genome. You can find the fasta file in ~/project/data/reference/Homo_sapiens.GRCh38.dna.chromosome.20.fa . Do it with a script called A02_create_bwa_index.sh . Also store this in the directory A-prepare_references . Answer A02_create_bwa_index.sh #!/usr/bin/env bash cd ~/project/data/reference/ bwa index Homo_sapiens.GRCh38.dna.chromosome.20.fa","title":"1. Download data and prepare the reference genome"},{"location":"day1/alignment/#2-read-alignment","text":"Check out the synopsis and manual of bwa mem . We\u2019ll be using paired-end reads of three samples that can be found at ~/project/data/fastq . If we run bwa mem with default options, which three arguments do we need? Answer The manual says: bwa mem [-aCHMpP] [-t nThreads] [-k minSeedLen] ... db.prefix reads.fq [mates.fq] So, we\u2019ll need: a database prefix ( db.prefix ) forward reads ( reads.fq ) reverse reads ( mates.fq ) For our reference sequence a command would look like: cd ~/project/ bwa mem \\ data/reference/Homo_sapiens.GRCh38.dna.chromosome.20.fa \\ <forward_reads.fq> \\ <reverse_reads.fq> \\ > <alignment.sam> We will now go through all the steps concerning alignment for the sample mother . To store the results of these steps, we will create a directory within ~/project called results . For the alignment, make a script called B01_alignment.sh . Since we will perform a similar analysis later on for all samples, we store this script in ~/project/scripts/B-mother_only . Your directory ~/project/scripts should now like this: scripts \u251c\u2500\u2500 A-prepare_references \u2502 \u251c\u2500\u2500 A01_download_course_data.sh \u2502 \u251c\u2500\u2500 A02_create_bwa_index.sh \u251c\u2500\u2500 B-mother_only \u2502 \u2514\u2500\u2500 B01_alignment.sh \u2514\u2500\u2500 C-all_samples In B01_alignment.sh write the commands to perform an alignment with bwa mem of the reads from the mother ( mother_R1.fastq and mother_R2.fastq ) against chromosome 20. Write the resulting .sam file to a directory in ~/project/results called alignments . Index prefix is the same a reference filename With default values, the name of the index of a reference for bwa mem is the same as the name of the reference itself. In this case, this would be Homo_sapiens.GRCh38.dna.chromosome.20.fa . Answer B01_read_alignment.sh #!/usr/bin/env bash cd ~/project/ mkdir -p results/alignments bwa mem \\ data/reference/Homo_sapiens.GRCh38.dna.chromosome.20.fa \\ data/fastq/mother_R1.fastq.gz \\ data/fastq/mother_R2.fastq.gz \\ > results/alignments/mother.sam","title":"2. Read alignment"},{"location":"day1/alignment/#3-alignment-statistics","text":"Exercise: Check out the statistics of the alignment by using samtools flagstat . Write the output of samtools flagstat to a file called mother.sam.flagstat . Do this by creating a script called B02_get_alignment_statistics.sh , and add this script to ~/project/scripts/B-mother_only . Find the documentation of samtools flagstat here . Any duplicates in there? Answer B02_get_alignment_statistics.sh #!/usr/bin/env bash cd ~/project/results/alignments samtools flagstat mother.sam > mother.sam.flagstat This should result in: 133477 + 0 in total (QC-passed reads + QC-failed reads) 0 + 0 secondary 317 + 0 supplementary 0 + 0 duplicates 132892 + 0 mapped (99.56% : N/A) 133160 + 0 paired in sequencing 66580 + 0 read1 66580 + 0 read2 131470 + 0 properly paired (98.73% : N/A) 131990 + 0 with itself and mate mapped 585 + 0 singletons (0.44% : N/A) 0 + 0 with mate mapped to a different chr 0 + 0 with mate mapped to a different chr (mapQ>=5) No duplicates were found ( 0 + 0 duplicates ). The aligner doesn\u2019t automatically flag duplicates. This needs to be done after the alignment.","title":"3. Alignment statistics"},{"location":"day1/alignment/#4-sorting-and-compression","text":"Many downstream analyses require a coordinate sorted alignment file. Now, your alignment file is in the same order as the fastq file. You can coordinate sort an alignment file with samtools sort . You can find the documentation here . Exercise : Sort the alignment file according to coordinate. In order to do this, create a script called B03_sort_alignment.sh (in ~/project/scripts/B-mother_only ). Answer B03_sort_alignment.sh #!/usr/bin/env bash cd ~/project/results samtools sort -o alignments/mother.sorted.sam alignments/mother.sam Tip: samtools sort and samtools view can write to stdout Like bwa mem , samtools sort and samtools view can write its output to stdout. This means that you need to redirect your output to a file with > or use the the output option -o . The command samtools view is very versatile. It takes an alignment file and writes a filtered or processed alignment to the output. You can for example use it to compress your SAM file into a BAM file. Let\u2019s start with that. Exercise : compress our SAM file into a BAM file and include the header in the output. For this, use the -b and -h options. Perform the calculation from a script called B04_compress_alignment.sh (in ~/project/scripts/B-mother_only ). Find the required documentation here . How much was the disk space reduced by compressing the file? Answer B04_compress_alignment.sh #!/usr/bin/env bash cd ~/project/results samtools view -bh alignments/mother.sorted.sam > alignments/mother.bam By using ls -lh , you can find out that mother.sorted.sam has a size of 55 Mb, while mother.bam is only 16 Mb.","title":"4. Sorting and compression"},{"location":"day1/alignment/#5-recap","text":"You have now performed: alignment sorting compression flag statistics On the sample mother . Your scripts directory should look like this: scripts \u251c\u2500\u2500 A-prepare_references \u2502 \u251c\u2500\u2500 A01_download_course_data.sh \u2502 \u2514\u2500\u2500 A02_create_bwa_index.sh \u251c\u2500\u2500 B-mother_only \u2502 \u251c\u2500\u2500 B01_alignment.sh \u2502 \u251c\u2500\u2500 B02_get_alignment_statistics.sh \u2502 \u251c\u2500\u2500 B03_sort_alignment.sh \u2502 \u2514\u2500\u2500 B04_compress_alignment.sh \u2514\u2500\u2500 C-all_samples","title":"5. Recap"},{"location":"day1/alignment_advanced/","text":"Learning outcomes After having completed this chapter you will be able to: Use samtools to mark duplicates from an alignment file Use samtools to add readgroups to an alignment file Use a for loop in bash to perform the same operation on a range of files Use samtools in a pipe to efficiently do multiple operations on an alignment file in a single command Material samtools documentation Exercises 1. Adding readgroups During several steps of variant calling gatk uses read group information. For each read, this gives information on the sequencing platform, the library, the lane and of course the sample. Have a look at the description of the different levels of read group information gatk uses here . Exercise: The documentation mentions several read group fields that are used by gatk . Have a look at the fastq header. Does that give you the information that is required? Do we have that information for our sample? Can you specify it for our sample? Hint You can have a look at the first few entries in the fastq file with: zcat mother_R1.fastq.gz | head Answer Most of the information you should now based on the experimental design, the rest you can find in the fastq header : PL : the platform. Should be quite obvious; you usually you have this information. For us, this would be ILLUMINA SM : the sample. All alignments that have reads coming from the same individual should have the same identifier in this field. For us, this would be mother . LB : library identifier. Molecular duplicates only exist within a library. If a single library was sequenced on multiple lanes, it is important to track this information. In our case, we have sequenced only one library, so you can specify it with e.g. lib1 . PU : platform unit. This field is used to identify the sequencing lane. The documentation tells us we should specify it as [FLOWCELL].[LANE].[SAMPLE BARCODE] . The header of the first entry in our fastq file looks like this: @H0164ALXX140820:2:1101:2136:40460/1 . Where the flowcell ID is H0164 and the lane 2 . This formatting is specific to Broad Genomic Services pipelines, and not very common nowadays. Here the sample barcode is added to the flowcell ID, and is therefore specified as ALXX140820. We can therefore specify it with H0164.2.ALXX140820 . ID : read group id. If you don\u2019t have specific information on the flowcell and lane (specified with PU ), you can use this field to specify a unique unit that is used for e.g. base quality score recalibration. This often a combination of a flow cell identifier and a lane. In our case this could be H0164.2 Note More modern output of an Illumina sequencer looks e.g. like this (example on Wikipedia ): @EAS139:136:FC706VJ:2:2104:15343:197393 1:Y:18:ATCACG Here, e.g. the PU field would be FC706VJ.2.ATCACG Exercise: Have a look at the documentation of AddOrReplaceReadGroups . Specify the required arguments, and run the command. Do this from a script called B05_add_readgroups.sh (in ~/project/scripts/B-mother_only ). Answer We can use the answers of the previous exercise, and use them in the command: B05_add_readgroups.sh #!/usr/bin/env bash cd ~/project/results gatk AddOrReplaceReadGroups \\ --INPUT alignments/mother.bam \\ --OUTPUT alignments/mother.rg.bam \\ --RGLB lib1 \\ --RGPU H0164.2.ALXX140820 \\ --RGPL ILLUMINA \\ --RGSM mother \\ --RGID H0164.2 Exercise: Compare the header and first alignments of mother.bam and mother.rg.bam . Notice any differences? Hint You can view the header with samtools view -H <alignment.bam> And the first few alignments with samtools view <alignment.bam> | head Answer Compared to the header of mother.markdup.bam , the header of mother.markdup.rg.bam contains an extra line starting with @RG : @RG ID:H0164.2 LB:lib1 PL:ILLUMINA SM:mother PU:H0164.2.ALXX140820 In the alignment records, a tag was added at the very end of each line: RG:Z:H0164.2 . Note that all fields ( LB , PU , etc.) are related to ID . So for each read only ID is specified and all other fields can be deducted from that. 2. Mark duplicates Now that we have specified read groups, we can mark the duplicates with gatk MarkDuplicates . Exercise: Have a look at the documentation , and run gatk MarkDuplicates with the three required arguments. Do this from a script called B06_mark_duplicates.sh (in ~/project/scripts/B-mother_only ). Answer B06_mark_duplicates.sh #!/usr/bin/env bash cd ~/project/results gatk MarkDuplicates \\ --INPUT alignments/mother.rg.bam \\ --OUTPUT alignments/mother.rg.md.bam \\ --METRICS_FILE alignments/marked_dup_metrics_mother.txt Exercise: Run samtools flagstat on the alignment file with marked duplicates, and write the output to a file called mother.rg.md.bam.flagstat . Create a script called B07_get_alignment_stats_after_md.sh (in ~/project/scripts/B-mother_only ). How many reads were marked as duplicate? Answer B07_get_alignment_stats_after_md.sh #!/usr/bin/env bash cd ~/project/results/alignments samtools flagstat mother.rg.md.bam > mother.rg.md.bam.flagstat Gives: 133477 + 0 in total (QC-passed reads + QC-failed reads) 0 + 0 secondary 317 + 0 supplementary 17329 + 0 duplicates 132892 + 0 mapped (99.56% : N/A) 133160 + 0 paired in sequencing 66580 + 0 read1 66580 + 0 read2 131470 + 0 properly paired (98.73% : N/A) 131990 + 0 with itself and mate mapped 585 + 0 singletons (0.44% : N/A) 0 + 0 with mate mapped to a different chr 0 + 0 with mate mapped to a different chr (mapQ>=5) Which tells us that 17329 reads were marked as duplicate. 3. Indexing To look up specific alignments, it is convenient to have your alignment file indexed. An indexing can be compared to a kind of \u2018phonebook\u2019 of your sequence alignment file. Indexing can be done with samtools as well, but it first needs to be sorted on coordinate (i.e. the alignment location). You can do it like this: samtools index <bam file> Exercise : Create a script called B08_index_alignment.sh (in ~/project/scripts/B-mother_only ) to perform the alignment. Answer B08_index_alignment.sh #!/usr/bin/env bash cd ~/project/results/alignments/ samtools index mother.rg.md.bam 4. Recap - mother only Now we have performed now the following steps on the sample mother : Read alignment Adding readgroups Marking of duplicates Indexing Your scripts directory should look like this: \u251c\u2500\u2500 A-prepare_references \u2502 \u251c\u2500\u2500 A01_download_course_data.sh \u2502 \u2514\u2500\u2500 A02_create_bwa_index.sh \u251c\u2500\u2500 B-mother_only \u2502 \u251c\u2500\u2500 B01_alignment.sh \u2502 \u251c\u2500\u2500 B02_get_alignment_statistics.sh \u2502 \u251c\u2500\u2500 B03_sort_alignment.sh \u2502 \u251c\u2500\u2500 B04_compress_alignment.sh \u2502 \u251c\u2500\u2500 B05_add_readgroups.sh \u2502 \u251c\u2500\u2500 B06_mark_duplicates.sh \u2502 \u251c\u2500\u2500 B07_get_alignment_stats_after_md.sh \u2502 \u2514\u2500\u2500 B08_index_alignment.sh \u2514\u2500\u2500 C-all_samples 5. Apply it on all three samples with pipes and loops We now apply these steps to all three samples. In order to do that, we combine the alignment with the sorting and compression in one command. We can do that with piping the output of bwa to samtools sort and samtools view , like this: SAMPLE = \"mother\" bwa mem data/reference/Homo_sapiens.GRCh38.dna.chromosome.20.fa \\ data/fastq/ \" $SAMPLE \" _R1.fastq.gz \\ data/fastq/ \" $SAMPLE \" _R2.fastq.gz \\ | samtools sort \\ | samtools view -bh > results/alignments/ \" $SAMPLE \" .bam Exercise : Make a directory in the scripts directory C-all_samples (so ~/project/scripts/C-all_samples ). In here, create a script called C01_alignment_sorting_compression.sh . Within that script use the above snippet to make a loop that performs the alignment, sorting and compression for all three samples (i.e. mother , father and son ). Answer Your scripts directory should look like: scripts \u251c\u2500\u2500 A-prepare_references \u2502 \u251c\u2500\u2500 A01_download_course_data.sh \u2502 \u2514\u2500\u2500 A02_create_bwa_index.sh \u251c\u2500\u2500 B-mother_only \u2502 \u251c\u2500\u2500 B01_alignment.sh \u2502 \u251c\u2500\u2500 B02_get_alignment_statistics.sh \u2502 \u251c\u2500\u2500 B03_sort_alignment.sh \u2502 \u251c\u2500\u2500 B04_compress_alignment.sh \u2502 \u251c\u2500\u2500 B05_add_readgroups.sh \u2502 \u251c\u2500\u2500 B06_mark_duplicates.sh \u2502 \u251c\u2500\u2500 B07_get_alignment_stats_after_md.sh \u2502 \u2514\u2500\u2500 B08_index_alignment.sh \u2514\u2500\u2500 C-all_samples \u2514\u2500\u2500 C01_alignment_sorting_compression.sh And the script: C01_alignment_sorting_compression.sh #!/usr/bin/env bash cd ~/project for SAMPLE in mother father son do bwa mem data/reference/Homo_sapiens.GRCh38.dna.chromosome.20.fa \\ data/fastq/ \" $SAMPLE \" _R1.fastq.gz \\ data/fastq/ \" $SAMPLE \" _R2.fastq.gz \\ | samtools sort \\ | samtools view -bh > results/alignments/ \" $SAMPLE \" .bam done Now we continue with adding the readgroups. For each sample, we have to add specific information to the different readgroup fields. We can do that by looping over a tab delimited file with sample-specific information in each row. Let\u2019s create that tab-delimited file. Exercise Generate a tab-delimited file called sample_rg_fields.txt and store it in ~/project/results/ . In this file, each line should represent a sample (mother, father and son), and you specify the SM , LB , PU and ID fields. E.g., the first line (for \u2018mother\u2019) would look like: mother lib1 H0164.2.ALXX140820 H0164.2 Warning Make sure to add a newline ( Enter ) at the end of the file. Otherwise a loop will stop at the second-last line. Answer Your file should look like this: mother lib1 H0164.2.ALXX140820 H0164.2 father lib2 H0164.3.ALXX140820 H0164.3 son lib3 H0164.6.ALXX140820 H0164.6 Exercise Generate a script called C02_add_readgroups.sh (in ~/project/scripts/C-all_samples ) to loop over the tab-delimited file (have a look at the last exercise in Setup ), and add the correct readgroups to the bam file of each sample with gatk AddOrReplaceReadGroups . Hint Try to just print the variables from a loop in order to check to see whether the loop performs according to your expectation. E.g.: cd ~/project/results cat sample_rg_fields.txt | while read SAMPLE LB PU ID do echo $SAMPLE $LB $PU $ID done Answer C02_add_readgroups.sh #!/usr/bin/env bash cd ~/project/results cat sample_rg_fields.txt | while read SAMPLE LB PU ID do gatk AddOrReplaceReadGroups \\ --INPUT alignments/ \" $SAMPLE \" .bam \\ --OUTPUT alignments/ \" $SAMPLE \" .rg.bam \\ --RGLB \" $LB \" \\ --RGPU \" $PU \" \\ --RGPL ILLUMINA \\ --RGSM \" $SAMPLE \" \\ --RGID \" $ID \" done As final step, we will mark the duplicates and perform the indexing for the three samples. Exercise: Generate two scripts called C03_mark_duplicates.sh and C04_index_alignments.sh , in which you loop over the sample names and perform the respective calculations. You can use B06_mark_duplicates.sh and B08_index_alignment.sh as a template. Answer C03_mark_duplicates.sh #!/usr/bin/env bash cd ~/project/results for SAMPLE in mother father son do gatk MarkDuplicates \\ --INPUT alignments/ \" $SAMPLE \" .rg.bam \\ --OUTPUT alignments/ \" $SAMPLE \" .rg.md.bam \\ --METRICS_FILE alignments/marked_dup_metrics_ \" $SAMPLE \" .txt done C04_index_alignment.sh #!/usr/bin/env bash cd ~/project/results for SAMPLE in mother father son do samtools index alignments/ \" $SAMPLE \" .rg.md.bam done 6. Recap - all samples Now, we have performed on all three samples: Alignment Sorting Compression Adding readgroups Marking of duplicates Indexing Your scripts directory should look like this: scripts \u251c\u2500\u2500 A-prepare_references \u2502 \u251c\u2500\u2500 A01_download_course_data.sh \u2502 \u2514\u2500\u2500 A02_create_bwa_index.sh \u251c\u2500\u2500 B-mother_only \u2502 \u251c\u2500\u2500 B01_alignment.sh \u2502 \u251c\u2500\u2500 B02_get_alignment_statistics.sh \u2502 \u251c\u2500\u2500 B03_sort_alignment.sh \u2502 \u251c\u2500\u2500 B04_compress_alignment.sh \u2502 \u251c\u2500\u2500 B05_add_readgroups.sh \u2502 \u251c\u2500\u2500 B06_mark_duplicates.sh \u2502 \u251c\u2500\u2500 B07_get_alignment_stats_after_md.sh \u2502 \u2514\u2500\u2500 B08_index_alignment.sh \u2514\u2500\u2500 C-all_samples \u251c\u2500\u2500 C01_alignment_sorting_compression.sh \u251c\u2500\u2500 C02_add_readgroups.sh \u251c\u2500\u2500 C03_mark_duplicates.sh \u2514\u2500\u2500 C04_index_alignment.sh","title":"Read alignment - advanced"},{"location":"day1/alignment_advanced/#learning-outcomes","text":"After having completed this chapter you will be able to: Use samtools to mark duplicates from an alignment file Use samtools to add readgroups to an alignment file Use a for loop in bash to perform the same operation on a range of files Use samtools in a pipe to efficiently do multiple operations on an alignment file in a single command","title":"Learning outcomes"},{"location":"day1/alignment_advanced/#material","text":"samtools documentation","title":"Material"},{"location":"day1/alignment_advanced/#exercises","text":"","title":"Exercises"},{"location":"day1/alignment_advanced/#1-adding-readgroups","text":"During several steps of variant calling gatk uses read group information. For each read, this gives information on the sequencing platform, the library, the lane and of course the sample. Have a look at the description of the different levels of read group information gatk uses here . Exercise: The documentation mentions several read group fields that are used by gatk . Have a look at the fastq header. Does that give you the information that is required? Do we have that information for our sample? Can you specify it for our sample? Hint You can have a look at the first few entries in the fastq file with: zcat mother_R1.fastq.gz | head Answer Most of the information you should now based on the experimental design, the rest you can find in the fastq header : PL : the platform. Should be quite obvious; you usually you have this information. For us, this would be ILLUMINA SM : the sample. All alignments that have reads coming from the same individual should have the same identifier in this field. For us, this would be mother . LB : library identifier. Molecular duplicates only exist within a library. If a single library was sequenced on multiple lanes, it is important to track this information. In our case, we have sequenced only one library, so you can specify it with e.g. lib1 . PU : platform unit. This field is used to identify the sequencing lane. The documentation tells us we should specify it as [FLOWCELL].[LANE].[SAMPLE BARCODE] . The header of the first entry in our fastq file looks like this: @H0164ALXX140820:2:1101:2136:40460/1 . Where the flowcell ID is H0164 and the lane 2 . This formatting is specific to Broad Genomic Services pipelines, and not very common nowadays. Here the sample barcode is added to the flowcell ID, and is therefore specified as ALXX140820. We can therefore specify it with H0164.2.ALXX140820 . ID : read group id. If you don\u2019t have specific information on the flowcell and lane (specified with PU ), you can use this field to specify a unique unit that is used for e.g. base quality score recalibration. This often a combination of a flow cell identifier and a lane. In our case this could be H0164.2 Note More modern output of an Illumina sequencer looks e.g. like this (example on Wikipedia ): @EAS139:136:FC706VJ:2:2104:15343:197393 1:Y:18:ATCACG Here, e.g. the PU field would be FC706VJ.2.ATCACG Exercise: Have a look at the documentation of AddOrReplaceReadGroups . Specify the required arguments, and run the command. Do this from a script called B05_add_readgroups.sh (in ~/project/scripts/B-mother_only ). Answer We can use the answers of the previous exercise, and use them in the command: B05_add_readgroups.sh #!/usr/bin/env bash cd ~/project/results gatk AddOrReplaceReadGroups \\ --INPUT alignments/mother.bam \\ --OUTPUT alignments/mother.rg.bam \\ --RGLB lib1 \\ --RGPU H0164.2.ALXX140820 \\ --RGPL ILLUMINA \\ --RGSM mother \\ --RGID H0164.2 Exercise: Compare the header and first alignments of mother.bam and mother.rg.bam . Notice any differences? Hint You can view the header with samtools view -H <alignment.bam> And the first few alignments with samtools view <alignment.bam> | head Answer Compared to the header of mother.markdup.bam , the header of mother.markdup.rg.bam contains an extra line starting with @RG : @RG ID:H0164.2 LB:lib1 PL:ILLUMINA SM:mother PU:H0164.2.ALXX140820 In the alignment records, a tag was added at the very end of each line: RG:Z:H0164.2 . Note that all fields ( LB , PU , etc.) are related to ID . So for each read only ID is specified and all other fields can be deducted from that.","title":"1. Adding readgroups"},{"location":"day1/alignment_advanced/#2-mark-duplicates","text":"Now that we have specified read groups, we can mark the duplicates with gatk MarkDuplicates . Exercise: Have a look at the documentation , and run gatk MarkDuplicates with the three required arguments. Do this from a script called B06_mark_duplicates.sh (in ~/project/scripts/B-mother_only ). Answer B06_mark_duplicates.sh #!/usr/bin/env bash cd ~/project/results gatk MarkDuplicates \\ --INPUT alignments/mother.rg.bam \\ --OUTPUT alignments/mother.rg.md.bam \\ --METRICS_FILE alignments/marked_dup_metrics_mother.txt Exercise: Run samtools flagstat on the alignment file with marked duplicates, and write the output to a file called mother.rg.md.bam.flagstat . Create a script called B07_get_alignment_stats_after_md.sh (in ~/project/scripts/B-mother_only ). How many reads were marked as duplicate? Answer B07_get_alignment_stats_after_md.sh #!/usr/bin/env bash cd ~/project/results/alignments samtools flagstat mother.rg.md.bam > mother.rg.md.bam.flagstat Gives: 133477 + 0 in total (QC-passed reads + QC-failed reads) 0 + 0 secondary 317 + 0 supplementary 17329 + 0 duplicates 132892 + 0 mapped (99.56% : N/A) 133160 + 0 paired in sequencing 66580 + 0 read1 66580 + 0 read2 131470 + 0 properly paired (98.73% : N/A) 131990 + 0 with itself and mate mapped 585 + 0 singletons (0.44% : N/A) 0 + 0 with mate mapped to a different chr 0 + 0 with mate mapped to a different chr (mapQ>=5) Which tells us that 17329 reads were marked as duplicate.","title":"2. Mark duplicates"},{"location":"day1/alignment_advanced/#3-indexing","text":"To look up specific alignments, it is convenient to have your alignment file indexed. An indexing can be compared to a kind of \u2018phonebook\u2019 of your sequence alignment file. Indexing can be done with samtools as well, but it first needs to be sorted on coordinate (i.e. the alignment location). You can do it like this: samtools index <bam file> Exercise : Create a script called B08_index_alignment.sh (in ~/project/scripts/B-mother_only ) to perform the alignment. Answer B08_index_alignment.sh #!/usr/bin/env bash cd ~/project/results/alignments/ samtools index mother.rg.md.bam","title":"3. Indexing"},{"location":"day1/alignment_advanced/#4-recap-mother-only","text":"Now we have performed now the following steps on the sample mother : Read alignment Adding readgroups Marking of duplicates Indexing Your scripts directory should look like this: \u251c\u2500\u2500 A-prepare_references \u2502 \u251c\u2500\u2500 A01_download_course_data.sh \u2502 \u2514\u2500\u2500 A02_create_bwa_index.sh \u251c\u2500\u2500 B-mother_only \u2502 \u251c\u2500\u2500 B01_alignment.sh \u2502 \u251c\u2500\u2500 B02_get_alignment_statistics.sh \u2502 \u251c\u2500\u2500 B03_sort_alignment.sh \u2502 \u251c\u2500\u2500 B04_compress_alignment.sh \u2502 \u251c\u2500\u2500 B05_add_readgroups.sh \u2502 \u251c\u2500\u2500 B06_mark_duplicates.sh \u2502 \u251c\u2500\u2500 B07_get_alignment_stats_after_md.sh \u2502 \u2514\u2500\u2500 B08_index_alignment.sh \u2514\u2500\u2500 C-all_samples","title":"4. Recap - mother only"},{"location":"day1/alignment_advanced/#5-apply-it-on-all-three-samples-with-pipes-and-loops","text":"We now apply these steps to all three samples. In order to do that, we combine the alignment with the sorting and compression in one command. We can do that with piping the output of bwa to samtools sort and samtools view , like this: SAMPLE = \"mother\" bwa mem data/reference/Homo_sapiens.GRCh38.dna.chromosome.20.fa \\ data/fastq/ \" $SAMPLE \" _R1.fastq.gz \\ data/fastq/ \" $SAMPLE \" _R2.fastq.gz \\ | samtools sort \\ | samtools view -bh > results/alignments/ \" $SAMPLE \" .bam Exercise : Make a directory in the scripts directory C-all_samples (so ~/project/scripts/C-all_samples ). In here, create a script called C01_alignment_sorting_compression.sh . Within that script use the above snippet to make a loop that performs the alignment, sorting and compression for all three samples (i.e. mother , father and son ). Answer Your scripts directory should look like: scripts \u251c\u2500\u2500 A-prepare_references \u2502 \u251c\u2500\u2500 A01_download_course_data.sh \u2502 \u2514\u2500\u2500 A02_create_bwa_index.sh \u251c\u2500\u2500 B-mother_only \u2502 \u251c\u2500\u2500 B01_alignment.sh \u2502 \u251c\u2500\u2500 B02_get_alignment_statistics.sh \u2502 \u251c\u2500\u2500 B03_sort_alignment.sh \u2502 \u251c\u2500\u2500 B04_compress_alignment.sh \u2502 \u251c\u2500\u2500 B05_add_readgroups.sh \u2502 \u251c\u2500\u2500 B06_mark_duplicates.sh \u2502 \u251c\u2500\u2500 B07_get_alignment_stats_after_md.sh \u2502 \u2514\u2500\u2500 B08_index_alignment.sh \u2514\u2500\u2500 C-all_samples \u2514\u2500\u2500 C01_alignment_sorting_compression.sh And the script: C01_alignment_sorting_compression.sh #!/usr/bin/env bash cd ~/project for SAMPLE in mother father son do bwa mem data/reference/Homo_sapiens.GRCh38.dna.chromosome.20.fa \\ data/fastq/ \" $SAMPLE \" _R1.fastq.gz \\ data/fastq/ \" $SAMPLE \" _R2.fastq.gz \\ | samtools sort \\ | samtools view -bh > results/alignments/ \" $SAMPLE \" .bam done Now we continue with adding the readgroups. For each sample, we have to add specific information to the different readgroup fields. We can do that by looping over a tab delimited file with sample-specific information in each row. Let\u2019s create that tab-delimited file. Exercise Generate a tab-delimited file called sample_rg_fields.txt and store it in ~/project/results/ . In this file, each line should represent a sample (mother, father and son), and you specify the SM , LB , PU and ID fields. E.g., the first line (for \u2018mother\u2019) would look like: mother lib1 H0164.2.ALXX140820 H0164.2 Warning Make sure to add a newline ( Enter ) at the end of the file. Otherwise a loop will stop at the second-last line. Answer Your file should look like this: mother lib1 H0164.2.ALXX140820 H0164.2 father lib2 H0164.3.ALXX140820 H0164.3 son lib3 H0164.6.ALXX140820 H0164.6 Exercise Generate a script called C02_add_readgroups.sh (in ~/project/scripts/C-all_samples ) to loop over the tab-delimited file (have a look at the last exercise in Setup ), and add the correct readgroups to the bam file of each sample with gatk AddOrReplaceReadGroups . Hint Try to just print the variables from a loop in order to check to see whether the loop performs according to your expectation. E.g.: cd ~/project/results cat sample_rg_fields.txt | while read SAMPLE LB PU ID do echo $SAMPLE $LB $PU $ID done Answer C02_add_readgroups.sh #!/usr/bin/env bash cd ~/project/results cat sample_rg_fields.txt | while read SAMPLE LB PU ID do gatk AddOrReplaceReadGroups \\ --INPUT alignments/ \" $SAMPLE \" .bam \\ --OUTPUT alignments/ \" $SAMPLE \" .rg.bam \\ --RGLB \" $LB \" \\ --RGPU \" $PU \" \\ --RGPL ILLUMINA \\ --RGSM \" $SAMPLE \" \\ --RGID \" $ID \" done As final step, we will mark the duplicates and perform the indexing for the three samples. Exercise: Generate two scripts called C03_mark_duplicates.sh and C04_index_alignments.sh , in which you loop over the sample names and perform the respective calculations. You can use B06_mark_duplicates.sh and B08_index_alignment.sh as a template. Answer C03_mark_duplicates.sh #!/usr/bin/env bash cd ~/project/results for SAMPLE in mother father son do gatk MarkDuplicates \\ --INPUT alignments/ \" $SAMPLE \" .rg.bam \\ --OUTPUT alignments/ \" $SAMPLE \" .rg.md.bam \\ --METRICS_FILE alignments/marked_dup_metrics_ \" $SAMPLE \" .txt done C04_index_alignment.sh #!/usr/bin/env bash cd ~/project/results for SAMPLE in mother father son do samtools index alignments/ \" $SAMPLE \" .rg.md.bam done","title":"5. Apply it on all three samples with pipes and loops"},{"location":"day1/alignment_advanced/#6-recap-all-samples","text":"Now, we have performed on all three samples: Alignment Sorting Compression Adding readgroups Marking of duplicates Indexing Your scripts directory should look like this: scripts \u251c\u2500\u2500 A-prepare_references \u2502 \u251c\u2500\u2500 A01_download_course_data.sh \u2502 \u2514\u2500\u2500 A02_create_bwa_index.sh \u251c\u2500\u2500 B-mother_only \u2502 \u251c\u2500\u2500 B01_alignment.sh \u2502 \u251c\u2500\u2500 B02_get_alignment_statistics.sh \u2502 \u251c\u2500\u2500 B03_sort_alignment.sh \u2502 \u251c\u2500\u2500 B04_compress_alignment.sh \u2502 \u251c\u2500\u2500 B05_add_readgroups.sh \u2502 \u251c\u2500\u2500 B06_mark_duplicates.sh \u2502 \u251c\u2500\u2500 B07_get_alignment_stats_after_md.sh \u2502 \u2514\u2500\u2500 B08_index_alignment.sh \u2514\u2500\u2500 C-all_samples \u251c\u2500\u2500 C01_alignment_sorting_compression.sh \u251c\u2500\u2500 C02_add_readgroups.sh \u251c\u2500\u2500 C03_mark_duplicates.sh \u2514\u2500\u2500 C04_index_alignment.sh","title":"6. Recap - all samples"},{"location":"day1/introduction/","text":"Learning outcomes After having completed this chapter you will be able to: Describe the importance of studying variants Define a DNA mutation Illustrate the difference between a somatic mutation and a germline mutation Describe the two major types of small variants: SNPs and INDELs Explain why SNPs are the most used type of variant for genetic research Explain what haplotypes are and how they can capture more genetic information compared to single small variants Material Course introduction: Download the presentation Introduction to variant analysis: Download the presentation","title":"Introduction"},{"location":"day1/introduction/#learning-outcomes","text":"After having completed this chapter you will be able to: Describe the importance of studying variants Define a DNA mutation Illustrate the difference between a somatic mutation and a germline mutation Describe the two major types of small variants: SNPs and INDELs Explain why SNPs are the most used type of variant for genetic research Explain what haplotypes are and how they can capture more genetic information compared to single small variants","title":"Learning outcomes"},{"location":"day1/introduction/#material","text":"Course introduction: Download the presentation Introduction to variant analysis: Download the presentation","title":"Material"},{"location":"day1/reproducibility/","text":"Learning outcomes After having completed this chapter you will be able to: Understand the importance of reproducibility Apply some basic rules to support reproducibilty in computational research Material Download the presentation Some good practices for reproducibility During the exercise you will be guided to adhere to the following basic principles for reproducibility: Execute the commands from a script in order to be able to trace back your steps Number scripts based on their order of execution (e.g. 01_download_reads.sh ) Give your scripts a descriptive and active name , e.g. 06_build_bowtie_index.sh Make your scripts specific , i.e. do not combine many different commands in the same script Refer to directories and variables at the beginning of the script Keep re-evaluating your code structure If you start a project it can be difficult to know what kind of analyses you are going to run, and how they interrelate. While working on a project, therefore re-evaluate readability of your project structure, and do not hesitate to change script numbering, names or contents. By adhering to these simple principles it will be relatively straightforward to re-do your analysis steps only based on the scripts, and will get you started to adhere to the Ten Simple Rules for Reproducible Computational Research . Exercises Throughout the exercises today and tomorrow we will work on three different \u2018subprojects\u2019: Preparing references by indexing Alignment and variant calling on one sample (\u2018mother\u2019) Alignment, variant calling and filtering on all samples We store the scripts required for these subprojects in different subdirectories of ~/project/scripts named: A-prepare_references B-mother_only C-all_samples You can already create these directories now with: cd ~/project/scripts/ mkdir -p \\ A-prepare_references \\ B-mother_only \\ C-all_samples By the end of day 2 ~/project/scripts should look (something) like this: scripts \u251c\u2500\u2500 A_prepare_references \u2502 \u251c\u2500\u2500 A01_download_course_data.sh \u2502 \u251c\u2500\u2500 A02_create_bwa_index.sh \u2502 \u251c\u2500\u2500 A03_create_vcf_indices.sh \u2502 \u2514\u2500\u2500 A04_create_fasta_index.sh \u251c\u2500\u2500 B_mother_only \u2502 \u251c\u2500\u2500 B01_alignment.sh \u2502 \u251c\u2500\u2500 B02_get_alignment_statistics.sh \u2502 \u251c\u2500\u2500 B03_sort_alignment.sh \u2502 \u251c\u2500\u2500 B04_compress_alignment.sh \u2502 \u251c\u2500\u2500 B05_add_readgroups.sh \u2502 \u251c\u2500\u2500 B06_mark_duplicates.sh \u2502 \u251c\u2500\u2500 B07_get_alignment_stats_after_md.sh \u2502 \u251c\u2500\u2500 B08_index_alignment.sh \u2502 \u251c\u2500\u2500 B09_perform_bqsr.sh \u2502 \u251c\u2500\u2500 B10_run_haplotypecaller.sh \u2502 \u2514\u2500\u2500 B11_variants_to_table.sh \u2514\u2500\u2500 C_all_samples \u251c\u2500\u2500 C01_alignment_sorting_compression.sh \u251c\u2500\u2500 C02_add_readgroups.sh \u251c\u2500\u2500 C03_mark_duplicates.sh \u251c\u2500\u2500 C04_index_alignment.sh \u251c\u2500\u2500 C05_perform_bqsr.sh \u251c\u2500\u2500 C06_run_haplotypecaller.sh \u251c\u2500\u2500 C07_create_genomicsdb.sh \u251c\u2500\u2500 C08_genotype_gvcfs.sh \u251c\u2500\u2500 C09_select_SNPs.sh \u251c\u2500\u2500 C10_select_INDELs.sh \u251c\u2500\u2500 C11_filter_SNPs.sh \u251c\u2500\u2500 C12_filter_INDELs.sh \u251c\u2500\u2500 C13_merge_filtered.sh \u251c\u2500\u2500 C14_extract_mother_only.sh \u251c\u2500\u2500 C15_evaluate_concordance.sh \u251c\u2500\u2500 C16_extract_mother_before_filtering.sh \u2514\u2500\u2500 C17_evaluate_concordance_before_filtering.sh","title":"Reproducibility"},{"location":"day1/reproducibility/#learning-outcomes","text":"After having completed this chapter you will be able to: Understand the importance of reproducibility Apply some basic rules to support reproducibilty in computational research","title":"Learning outcomes"},{"location":"day1/reproducibility/#material","text":"Download the presentation","title":"Material"},{"location":"day1/reproducibility/#some-good-practices-for-reproducibility","text":"During the exercise you will be guided to adhere to the following basic principles for reproducibility: Execute the commands from a script in order to be able to trace back your steps Number scripts based on their order of execution (e.g. 01_download_reads.sh ) Give your scripts a descriptive and active name , e.g. 06_build_bowtie_index.sh Make your scripts specific , i.e. do not combine many different commands in the same script Refer to directories and variables at the beginning of the script Keep re-evaluating your code structure If you start a project it can be difficult to know what kind of analyses you are going to run, and how they interrelate. While working on a project, therefore re-evaluate readability of your project structure, and do not hesitate to change script numbering, names or contents. By adhering to these simple principles it will be relatively straightforward to re-do your analysis steps only based on the scripts, and will get you started to adhere to the Ten Simple Rules for Reproducible Computational Research .","title":"Some good practices for reproducibility"},{"location":"day1/reproducibility/#exercises","text":"Throughout the exercises today and tomorrow we will work on three different \u2018subprojects\u2019: Preparing references by indexing Alignment and variant calling on one sample (\u2018mother\u2019) Alignment, variant calling and filtering on all samples We store the scripts required for these subprojects in different subdirectories of ~/project/scripts named: A-prepare_references B-mother_only C-all_samples You can already create these directories now with: cd ~/project/scripts/ mkdir -p \\ A-prepare_references \\ B-mother_only \\ C-all_samples By the end of day 2 ~/project/scripts should look (something) like this: scripts \u251c\u2500\u2500 A_prepare_references \u2502 \u251c\u2500\u2500 A01_download_course_data.sh \u2502 \u251c\u2500\u2500 A02_create_bwa_index.sh \u2502 \u251c\u2500\u2500 A03_create_vcf_indices.sh \u2502 \u2514\u2500\u2500 A04_create_fasta_index.sh \u251c\u2500\u2500 B_mother_only \u2502 \u251c\u2500\u2500 B01_alignment.sh \u2502 \u251c\u2500\u2500 B02_get_alignment_statistics.sh \u2502 \u251c\u2500\u2500 B03_sort_alignment.sh \u2502 \u251c\u2500\u2500 B04_compress_alignment.sh \u2502 \u251c\u2500\u2500 B05_add_readgroups.sh \u2502 \u251c\u2500\u2500 B06_mark_duplicates.sh \u2502 \u251c\u2500\u2500 B07_get_alignment_stats_after_md.sh \u2502 \u251c\u2500\u2500 B08_index_alignment.sh \u2502 \u251c\u2500\u2500 B09_perform_bqsr.sh \u2502 \u251c\u2500\u2500 B10_run_haplotypecaller.sh \u2502 \u2514\u2500\u2500 B11_variants_to_table.sh \u2514\u2500\u2500 C_all_samples \u251c\u2500\u2500 C01_alignment_sorting_compression.sh \u251c\u2500\u2500 C02_add_readgroups.sh \u251c\u2500\u2500 C03_mark_duplicates.sh \u251c\u2500\u2500 C04_index_alignment.sh \u251c\u2500\u2500 C05_perform_bqsr.sh \u251c\u2500\u2500 C06_run_haplotypecaller.sh \u251c\u2500\u2500 C07_create_genomicsdb.sh \u251c\u2500\u2500 C08_genotype_gvcfs.sh \u251c\u2500\u2500 C09_select_SNPs.sh \u251c\u2500\u2500 C10_select_INDELs.sh \u251c\u2500\u2500 C11_filter_SNPs.sh \u251c\u2500\u2500 C12_filter_INDELs.sh \u251c\u2500\u2500 C13_merge_filtered.sh \u251c\u2500\u2500 C14_extract_mother_only.sh \u251c\u2500\u2500 C15_evaluate_concordance.sh \u251c\u2500\u2500 C16_extract_mother_before_filtering.sh \u2514\u2500\u2500 C17_evaluate_concordance_before_filtering.sh","title":"Exercises"},{"location":"day1/server_login/","text":"Learning outcomes Note You might already be able to do some or all of these learning outcomes. If so, you can go through the corresponding exercises quickly. The general aim of this chapter is to work comfortably on a remote server by using the command line. After having completed this chapter you will be able to: Use the command line to: Make a directory Change file permissions to \u2018executable\u2019 Run a bash script Pipe data from and to a file or other executable Program a loop in bash Choose your platform In this part we will show you how to access the cloud server, or setup your computer to do the exercises with conda or with Docker. If you are doing the course with a teacher , you will have to login to the remote server. Therefore choose: Cloud notebook If you are doing this course independently (i.e. without a teacher) choose either: conda Docker Cloud server Docker conda Exercises First login If you are participating in this course with a teacher, you have received a link and a password. Copy-paste the link (including the port, e.g.: http://12.345.678.91:10002 ) in your browser. This should result in the following page: Info The link gives you access to a web version of Visual Studio Code . This is a powerful code editor that you can also use a local application on your computer. Type in the password that was provided to you by the teacher. Now let\u2019s open the terminal. You can do that with Ctrl + ` . Or by clicking Application menu > Terminal > New Terminal : For a.o. efficiency and reproducibility it makes sense to execute your commands from a script. With use of the \u2018new file\u2019 button: Material Instructions to install docker Instructions to set up to container Exercises First login Docker can be used to run an entire isolated environment in a container. This means that we can run the software with all its dependencies required for this course locally in your computer. Independent of your operating system. In the video below there\u2019s a tutorial on how to set up a docker container for this course. Note that you will need administrator rights, and that if you are using Windows, you need the latest version of Windows 10. The command to run the environment required for this course looks like this (in a terminal): Modify the script Modify the path after -v to the working directory on your computer before running it. docker run \\ --rm \\ -p 8443 :8443 \\ -e PUID = 1000 \\ -e PGID = 1000 \\ -e DEFAULT_WORKSPACE = /config/project \\ -v $PWD :/config/project \\ geertvangeest/ngs-variants-vscode If this command has run successfully, use your browser to navigate to port 8443 on your local machine: http://127.0.0.1:8443 The option -v mounts a local directory in your computer to the directory /config/project in the docker container. In that way, you have files available both in the container and on your computer. Use this directory on your computer to e.g. visualise data with IGV. Change the first path to a path on your computer that you want to use as a working directory. Don\u2019t mount directly in the home dir Don\u2019t directly mount your local directory to the home directory ( /root ). This will lead to unexpected behaviour. The part geertvangeest/ngs-variants-vscode is the image we are going to load into the container. The image contains all the information about software and dependencies needed for this course. When you run this command for the first time it will download the image. Once it\u2019s on your computer, it will start immediately. If you have a conda installation on your local computer, you can install the required software using conda. You can build the environment from environment.yml Generate the conda environment like this: conda env create --name ngs-tools -f environment.yml The yaml file probably only works for Linux systems If you want to use the conda environment on a different OS, use: conda create -n ngs-tools python = 3 .8 conda activate ngs-tools conda install -y -c bioconda \\ samtools \\ bwa \\ snpeff \\ gatk4 \\ r-base This will create the conda environment ngs-tools Activate it like so: conda activate ngs-tools After successful installation and activating the environment all the software required to do the exercises should be available. A UNIX command line interface (CLI) refresher Most bioinformatics software are UNIX based and are executed through the CLI. When working with NGS data, it is therefore convenient to improve your knowledge on UNIX. For this course, we need basic understanding of UNIX CLI, so here are some exercises to refresh your memory. If you need some reminders of the commands, here\u2019s a link to a UNIX command line cheat sheet: UNIX cheat sheet Make a new directory Make a directory scripts within ~/project and make it your current directory. Answer cd ~/project mkdir scripts cd scripts File permissions Generate an empty script in your newly made directory ~/project/scripts like this: touch new_script.sh Add a command to this script that writes \u201cSIB courses are great!\u201d (or something you can better relate to.. ) to stdout, and try to run it. Answer Generate a script as described above. The script should look like this: #!/usr/bin/env bash echo \"SIB courses are great!\" Usually, you can run it like this: ./new_script.sh But there\u2019s an error: bash: ./new_script.sh: Permission denied Why is there an error? Hint Use ls -lh new_script.sh to check the permissions. Answer ls -lh new_script.sh gives: -rw-r--r-- 1 user group 51B Nov 11 16 :21 new_script.sh There\u2019s no x in the permissions string. You should change at least the permissions of the user. Make the script executable for yourself, and run it. Answer Change permissions: chmod u+x new_script.sh ls -lh new_script.sh now gives: -rwxr--r-- 1 user group 51B Nov 11 16:21 new_script.sh So it should be executable: ./new_script.sh More on chmod and file permissions here . Redirection: > and | In the root directory (go there like this: cd / ) there are a range of system directories and files. Write the names of all directories and files to a file called system_dirs.txt in your working directory. Answer ls / > ~/project/system_dirs.txt The command wc -l counts the number of lines, and can read from stdin. Make a one-liner with a pipe | symbol to find out how many system directories and files there are. Answer ls / | wc -l Variables Store system_dirs.txt as variable (like this: VAR=variable ), and use wc -l on that variable to count the number of lines in the file. Answer FILE = ~/project/system_dirs.txt wc -l $FILE shell scripts Make a shell script that automatically counts the number of system directories and files. Answer Make a script called e.g. current_system_dirs.sh : #!/usr/bin/env bash cd / ls | wc -l Loops If you want to run the same command on a range of arguments, it\u2019s not very convenient to type the command for each individual argument. For example, you could write dog , fox , bird to stdout in a script like this: #!/usr/bin/env bash echo dog echo fox echo bird However, if you want to change the command (add an option for example), you would have to change it for all the three command calls. Amongst others for that reason, you want to write the command only once. You can do this with a for-loop, like this: #!/usr/bin/env bash ANIMALS = \"dog fox bird\" for animal in $ANIMALS do echo $animal done Which results in: dog fox bird Write a shell script that removes all the letters \u201ce\u201d from a list of words. Hint Removing the letter \u201ce\u201d from a string can be done with tr like this: word = \"test\" echo $word | tr -d \"e\" Which would result in: tst Answer Your script should e.g. look like this (I\u2019ve added some awesome functionality): #!/usr/bin/env bash WORDLIST = \"here is a list of words resulting in a sentence\" for word in $WORDLIST do echo \"' $word ' with e's removed looks like:\" echo $word | tr -d \"e\" done resulting in: 'here' with e's removed looks like: hr 'is' with e's removed looks like: is 'a' with e's removed looks like: a 'list' with e's removed looks like: list 'of' with e's removed looks like: of 'words' with e's removed looks like: words 'resulting' with e's removed looks like: rsulting 'in' with e's removed looks like: in 'a' with e's removed looks like: a 'sentence' with e's removed looks like: sntnc Like you might be used to in R or python you can also loop over lines in files. This can be convenient if you have for example a set of parameters in each line of a file. Create a tab-delimited file animals.txt with the following contents: dog retrieves 4 fox jumps 4 bird flies 2 Hint If you\u2019re having trouble typing the actual \u2018tabs\u2019 you can also download the file here With unix shell you can loop over the lines of that file and store each column as a variable. Below, the three columns in the tab delimited file are stored in the variables $animal , $behaviour and $leg_number : cat animals.txt | while read animal behaviour leg_number do #something here done Exercise: Modify the script in such a way that it writes the strings that are stored in the variables at each line to stdout. Done cat animals.txt | while read animal behaviour leg_number do echo \"The $animal $behaviour , and has $leg_number legs\" done","title":"Setup"},{"location":"day1/server_login/#learning-outcomes","text":"Note You might already be able to do some or all of these learning outcomes. If so, you can go through the corresponding exercises quickly. The general aim of this chapter is to work comfortably on a remote server by using the command line. After having completed this chapter you will be able to: Use the command line to: Make a directory Change file permissions to \u2018executable\u2019 Run a bash script Pipe data from and to a file or other executable Program a loop in bash Choose your platform In this part we will show you how to access the cloud server, or setup your computer to do the exercises with conda or with Docker. If you are doing the course with a teacher , you will have to login to the remote server. Therefore choose: Cloud notebook If you are doing this course independently (i.e. without a teacher) choose either: conda Docker Cloud server Docker conda","title":"Learning outcomes"},{"location":"day1/server_login/#exercises","text":"","title":"Exercises"},{"location":"day1/server_login/#first-login","text":"If you are participating in this course with a teacher, you have received a link and a password. Copy-paste the link (including the port, e.g.: http://12.345.678.91:10002 ) in your browser. This should result in the following page: Info The link gives you access to a web version of Visual Studio Code . This is a powerful code editor that you can also use a local application on your computer. Type in the password that was provided to you by the teacher. Now let\u2019s open the terminal. You can do that with Ctrl + ` . Or by clicking Application menu > Terminal > New Terminal : For a.o. efficiency and reproducibility it makes sense to execute your commands from a script. With use of the \u2018new file\u2019 button:","title":"First login"},{"location":"day1/server_login/#material","text":"Instructions to install docker Instructions to set up to container","title":"Material"},{"location":"day1/server_login/#exercises_1","text":"","title":"Exercises"},{"location":"day1/server_login/#first-login_1","text":"Docker can be used to run an entire isolated environment in a container. This means that we can run the software with all its dependencies required for this course locally in your computer. Independent of your operating system. In the video below there\u2019s a tutorial on how to set up a docker container for this course. Note that you will need administrator rights, and that if you are using Windows, you need the latest version of Windows 10. The command to run the environment required for this course looks like this (in a terminal): Modify the script Modify the path after -v to the working directory on your computer before running it. docker run \\ --rm \\ -p 8443 :8443 \\ -e PUID = 1000 \\ -e PGID = 1000 \\ -e DEFAULT_WORKSPACE = /config/project \\ -v $PWD :/config/project \\ geertvangeest/ngs-variants-vscode If this command has run successfully, use your browser to navigate to port 8443 on your local machine: http://127.0.0.1:8443 The option -v mounts a local directory in your computer to the directory /config/project in the docker container. In that way, you have files available both in the container and on your computer. Use this directory on your computer to e.g. visualise data with IGV. Change the first path to a path on your computer that you want to use as a working directory. Don\u2019t mount directly in the home dir Don\u2019t directly mount your local directory to the home directory ( /root ). This will lead to unexpected behaviour. The part geertvangeest/ngs-variants-vscode is the image we are going to load into the container. The image contains all the information about software and dependencies needed for this course. When you run this command for the first time it will download the image. Once it\u2019s on your computer, it will start immediately. If you have a conda installation on your local computer, you can install the required software using conda. You can build the environment from environment.yml Generate the conda environment like this: conda env create --name ngs-tools -f environment.yml The yaml file probably only works for Linux systems If you want to use the conda environment on a different OS, use: conda create -n ngs-tools python = 3 .8 conda activate ngs-tools conda install -y -c bioconda \\ samtools \\ bwa \\ snpeff \\ gatk4 \\ r-base This will create the conda environment ngs-tools Activate it like so: conda activate ngs-tools After successful installation and activating the environment all the software required to do the exercises should be available.","title":"First login"},{"location":"day1/server_login/#a-unix-command-line-interface-cli-refresher","text":"Most bioinformatics software are UNIX based and are executed through the CLI. When working with NGS data, it is therefore convenient to improve your knowledge on UNIX. For this course, we need basic understanding of UNIX CLI, so here are some exercises to refresh your memory. If you need some reminders of the commands, here\u2019s a link to a UNIX command line cheat sheet: UNIX cheat sheet","title":"A UNIX command line interface (CLI) refresher"},{"location":"day1/server_login/#make-a-new-directory","text":"Make a directory scripts within ~/project and make it your current directory. Answer cd ~/project mkdir scripts cd scripts","title":"Make a new directory"},{"location":"day1/server_login/#file-permissions","text":"Generate an empty script in your newly made directory ~/project/scripts like this: touch new_script.sh Add a command to this script that writes \u201cSIB courses are great!\u201d (or something you can better relate to.. ) to stdout, and try to run it. Answer Generate a script as described above. The script should look like this: #!/usr/bin/env bash echo \"SIB courses are great!\" Usually, you can run it like this: ./new_script.sh But there\u2019s an error: bash: ./new_script.sh: Permission denied Why is there an error? Hint Use ls -lh new_script.sh to check the permissions. Answer ls -lh new_script.sh gives: -rw-r--r-- 1 user group 51B Nov 11 16 :21 new_script.sh There\u2019s no x in the permissions string. You should change at least the permissions of the user. Make the script executable for yourself, and run it. Answer Change permissions: chmod u+x new_script.sh ls -lh new_script.sh now gives: -rwxr--r-- 1 user group 51B Nov 11 16:21 new_script.sh So it should be executable: ./new_script.sh More on chmod and file permissions here .","title":"File permissions"},{"location":"day1/server_login/#redirection-and","text":"In the root directory (go there like this: cd / ) there are a range of system directories and files. Write the names of all directories and files to a file called system_dirs.txt in your working directory. Answer ls / > ~/project/system_dirs.txt The command wc -l counts the number of lines, and can read from stdin. Make a one-liner with a pipe | symbol to find out how many system directories and files there are. Answer ls / | wc -l","title":"Redirection: &gt; and |"},{"location":"day1/server_login/#variables","text":"Store system_dirs.txt as variable (like this: VAR=variable ), and use wc -l on that variable to count the number of lines in the file. Answer FILE = ~/project/system_dirs.txt wc -l $FILE","title":"Variables"},{"location":"day1/server_login/#shell-scripts","text":"Make a shell script that automatically counts the number of system directories and files. Answer Make a script called e.g. current_system_dirs.sh : #!/usr/bin/env bash cd / ls | wc -l","title":"shell scripts"},{"location":"day1/server_login/#loops","text":"If you want to run the same command on a range of arguments, it\u2019s not very convenient to type the command for each individual argument. For example, you could write dog , fox , bird to stdout in a script like this: #!/usr/bin/env bash echo dog echo fox echo bird However, if you want to change the command (add an option for example), you would have to change it for all the three command calls. Amongst others for that reason, you want to write the command only once. You can do this with a for-loop, like this: #!/usr/bin/env bash ANIMALS = \"dog fox bird\" for animal in $ANIMALS do echo $animal done Which results in: dog fox bird Write a shell script that removes all the letters \u201ce\u201d from a list of words. Hint Removing the letter \u201ce\u201d from a string can be done with tr like this: word = \"test\" echo $word | tr -d \"e\" Which would result in: tst Answer Your script should e.g. look like this (I\u2019ve added some awesome functionality): #!/usr/bin/env bash WORDLIST = \"here is a list of words resulting in a sentence\" for word in $WORDLIST do echo \"' $word ' with e's removed looks like:\" echo $word | tr -d \"e\" done resulting in: 'here' with e's removed looks like: hr 'is' with e's removed looks like: is 'a' with e's removed looks like: a 'list' with e's removed looks like: list 'of' with e's removed looks like: of 'words' with e's removed looks like: words 'resulting' with e's removed looks like: rsulting 'in' with e's removed looks like: in 'a' with e's removed looks like: a 'sentence' with e's removed looks like: sntnc Like you might be used to in R or python you can also loop over lines in files. This can be convenient if you have for example a set of parameters in each line of a file. Create a tab-delimited file animals.txt with the following contents: dog retrieves 4 fox jumps 4 bird flies 2 Hint If you\u2019re having trouble typing the actual \u2018tabs\u2019 you can also download the file here With unix shell you can loop over the lines of that file and store each column as a variable. Below, the three columns in the tab delimited file are stored in the variables $animal , $behaviour and $leg_number : cat animals.txt | while read animal behaviour leg_number do #something here done Exercise: Modify the script in such a way that it writes the strings that are stored in the variables at each line to stdout. Done cat animals.txt | while read animal behaviour leg_number do echo \"The $animal $behaviour , and has $leg_number legs\" done","title":"Loops"},{"location":"day2/annotation/","text":"Learning outcomes After having completed this chapter you will be able to: Describe the aims of variant annotation Explain how variants are ranked in order of importance Explain how splice variation affects variant annotation Perform a variant annotation with snpEff Interpret the report generated by snpEff Explain how variant annotation can be added to a vcf file Material Download the presentation Exercises To use the human genome as a reference, we have downloaded the database with: No need to download, it\u2019s already downloaded for you # don't run this. It's already downloaded for you snpEff download -v GRCh38.99 You can run snpEff like so: mkdir annotation snpEff -Xmx4g \\ -v \\ -dataDir /data/ \\ GRCh38.99 \\ <filtered_variants.vcf> \\ > <annotated_variants.vcf> Exercise: Run the command on the filtered vcf ( trio.filtered.vcf ) using a script called C18_annotate_snpEff.sh . Check out the html file ( snpEff_summary.html ). Try to answer these questions: A. How many effects were calculated? B. How many variants are in the vcf? C. Why is this different? D. How many effects result in a missense mutation? Answer Your script: C18_annotate_snpEff.sh #!/usr/bin/env bash cd ~/project/results/variants snpEff -Xmx4g \\ -v \\ -dataDir /data/ \\ GRCh38.99 \\ trio.filtered.vcf \\ > trio.filtered.snpeff.vcf A. There were 10,357 effects calculated. B. There are only 556 variants in the vcf. C. This means that there are multiple effects per variant. snpEff calculates effects for each splice variant, and therefore the number of effects are a multitude of the number of variants. D. Two effects result in a missense mutation. You can (quick and dirty) query the annotated vcf for the missense mutation with grep . Exercise: Find the variant causing the missense mutation (the line contains the string missense ). And answer the following questions: Hint grep missense trio.filtered.snpeff.vcf Run the command and have a look at the SnpEff ANN field documentation . Answer the following questions: A. How are the SNP annotations stored in the vcf? B. What are the genotypes of the individuals? C. Which amino acid change does it cause? Answer Find the line with the missense mutation like this: grep missense annotation/trio.filtered.snpeff.vcf This results in (truncated long line, scroll to the right to see more): chr20 10049540 . T A 220.29 PASS AC=1;AF=0.167;AN=6;BaseQRankSum=-6.040e-01;DP=85;ExcessHet=3.0103;FS=0.000;MLEAC=1;MLEAF=0.167;MQ=60.00;MQRankSum=0.00;QD=8.16;ReadPosRankSum=0.226;SOR=0.951;ANN=A|missense_variant|MODERATE|ANKEF1|ENSG00000132623|transcript|ENST00000378392.6|protein_coding|7/11|c.971T>A|p.Leu324Gln|1426/5429|971/2331|324/776||,A|missense_variant|MODERATE|ANKEF1|ENSG00000132623|transcript|ENST00000378380.4|protein_coding|6/10|c.971T>A|p.Leu324Gln|1300/5303|971/2331|324/776|| GT:AD:DP:GQ:PL 0/0:34,0:34:99:0,102,1163 0/1:17,10:27:99:229,0,492 0/0:24,0:24:72:0,72,811 A. SNP annotations are stored in the INFO field, starting with ANN= B. The genotypes are homozygous reference for the father and son, and heterozygous for the mother. (find the order of the samples with grep ^#CHROM ) C. The triplet changes from cTg to cAg, resulting in a change from Leu (Leucine) to Gln (Glutamine).","title":"Annotation"},{"location":"day2/annotation/#learning-outcomes","text":"After having completed this chapter you will be able to: Describe the aims of variant annotation Explain how variants are ranked in order of importance Explain how splice variation affects variant annotation Perform a variant annotation with snpEff Interpret the report generated by snpEff Explain how variant annotation can be added to a vcf file","title":"Learning outcomes"},{"location":"day2/annotation/#material","text":"Download the presentation","title":"Material"},{"location":"day2/annotation/#exercises","text":"To use the human genome as a reference, we have downloaded the database with: No need to download, it\u2019s already downloaded for you # don't run this. It's already downloaded for you snpEff download -v GRCh38.99 You can run snpEff like so: mkdir annotation snpEff -Xmx4g \\ -v \\ -dataDir /data/ \\ GRCh38.99 \\ <filtered_variants.vcf> \\ > <annotated_variants.vcf> Exercise: Run the command on the filtered vcf ( trio.filtered.vcf ) using a script called C18_annotate_snpEff.sh . Check out the html file ( snpEff_summary.html ). Try to answer these questions: A. How many effects were calculated? B. How many variants are in the vcf? C. Why is this different? D. How many effects result in a missense mutation? Answer Your script: C18_annotate_snpEff.sh #!/usr/bin/env bash cd ~/project/results/variants snpEff -Xmx4g \\ -v \\ -dataDir /data/ \\ GRCh38.99 \\ trio.filtered.vcf \\ > trio.filtered.snpeff.vcf A. There were 10,357 effects calculated. B. There are only 556 variants in the vcf. C. This means that there are multiple effects per variant. snpEff calculates effects for each splice variant, and therefore the number of effects are a multitude of the number of variants. D. Two effects result in a missense mutation. You can (quick and dirty) query the annotated vcf for the missense mutation with grep . Exercise: Find the variant causing the missense mutation (the line contains the string missense ). And answer the following questions: Hint grep missense trio.filtered.snpeff.vcf Run the command and have a look at the SnpEff ANN field documentation . Answer the following questions: A. How are the SNP annotations stored in the vcf? B. What are the genotypes of the individuals? C. Which amino acid change does it cause? Answer Find the line with the missense mutation like this: grep missense annotation/trio.filtered.snpeff.vcf This results in (truncated long line, scroll to the right to see more): chr20 10049540 . T A 220.29 PASS AC=1;AF=0.167;AN=6;BaseQRankSum=-6.040e-01;DP=85;ExcessHet=3.0103;FS=0.000;MLEAC=1;MLEAF=0.167;MQ=60.00;MQRankSum=0.00;QD=8.16;ReadPosRankSum=0.226;SOR=0.951;ANN=A|missense_variant|MODERATE|ANKEF1|ENSG00000132623|transcript|ENST00000378392.6|protein_coding|7/11|c.971T>A|p.Leu324Gln|1426/5429|971/2331|324/776||,A|missense_variant|MODERATE|ANKEF1|ENSG00000132623|transcript|ENST00000378380.4|protein_coding|6/10|c.971T>A|p.Leu324Gln|1300/5303|971/2331|324/776|| GT:AD:DP:GQ:PL 0/0:34,0:34:99:0,102,1163 0/1:17,10:27:99:229,0,492 0/0:24,0:24:72:0,72,811 A. SNP annotations are stored in the INFO field, starting with ANN= B. The genotypes are homozygous reference for the father and son, and heterozygous for the mother. (find the order of the samples with grep ^#CHROM ) C. The triplet changes from cTg to cAg, resulting in a change from Leu (Leucine) to Gln (Glutamine).","title":"Exercises"},{"location":"day2/filtering_evaluation/","text":"Learning outcomes After having completed this chapter you will be able to: Explain why using Variant Quality Score Recalibration (VQSR) for filtering variants can outperform hard filtering Perform hard filtering on both SNPs and INDELs separately by using gatk SelectVariants in combination with gatk VariantFiltration Perform concordance between called variants and a truth set and evaluate performance of a variant calling workflow Material Download the presentation Exercises 1. Hard filtering The developers of gatk strongly advise to do Variant Quality Score Recalibration (VQSR) for filtering SNPs and INDELs. However, this is not always possible. For example, in the case of limited data availability and/or in the case you are working with non-model organisms and/or in the case you are a bit lazy and okay with a number of false positives. Our dataset is too small to apply VQSR. We will therefore do hard filtering instead. Splitting SNPs and INDELs First, filtering thresholds are usually different for SNPs and INDELs. Therefore, we will split trio.vcf into two vcfs, one containg only SNPs, and one containing only INDELs. You can extract all the SNP records in our trio vcf like this: cd ~/project/results gatk SelectVariants \\ --variant variants/trio.vcf \\ --select-type-to-include SNP \\ --output variants/trio.SNP.vcf Exercise: Check out the documentation of gatk SelectVariants , and: Figure out what you\u2019ll need to write at --select-type-to-include if you want to select only INDELS. Make a script (named C09_select_SNPs.sh ) to generate a vcf with only the SNPs Make a script (named C10_select_INDELs.sh ) to generate a second vcf with only the INDELs from trio.vcf . Answer You will need to fill in INDEL at --select-type-to-include to filter for INDELs. To get the SNPs you can run the command above: C09_select_SNPs.sh #!/usr/bin/env bash cd ~/project gatk SelectVariants \\ --variant results/variants/trio.vcf \\ --select-type-to-include SNP \\ --output results/variants/trio.SNP.vcf To get the INDELs you\u2019ll need to change --select-type-to-include to INDEL : C10_select_INDELs.sh #!/usr/bin/env bash cd ~/project gatk SelectVariants \\ --variant results/variants/trio.vcf \\ --select-type-to-include INDEL \\ --output results/variants/trio.INDEL.vcf Filtering SNPs The command gatk VariantFiltration enables you to filter for both the INFO field (per variant) and FORMAT field (per genotype). For now we\u2019re only interested in filtering variants. Below you can find the command to hard-filter the SNP variants on some sensible thresholds (that are explained here ). gatk VariantFiltration \\ --variant variants/trio.SNP.vcf \\ --filter-expression \"QD < 2.0\" --filter-name \"QD2\" \\ --filter-expression \"QUAL < 30.0\" --filter-name \"QUAL30\" \\ --filter-expression \"SOR > 3.0\" --filter-name \"SOR3\" \\ --filter-expression \"FS > 60.0\" --filter-name \"FS60\" \\ --filter-expression \"MQ < 40.0\" --filter-name \"MQ40\" \\ --filter-expression \"MQRankSum < -12.5\" --filter-name \"MQRankSum-12.5\" \\ --filter-expression \"ReadPosRankSum < -8.0\" --filter-name \"ReadPosRankSum-8\" \\ --output variants/trio.SNP.filtered.vcf Exercise: Run the filtering command above in a script called C11_filter_SNPs.sh . Did it affect the number of records in the vcf? Hint You can check out the number of records in a vcf with: grep -v \"^#\" <variants.vcf> | wc -l Answer Your script: C11_filter_SNPs.sh #!/usr/bin/env bash cd ~/project/results/variants gatk VariantFiltration \\ --variant trio.SNP.vcf \\ --filter-expression \"QD < 2.0\" --filter-name \"QD2\" \\ --filter-expression \"QUAL < 30.0\" --filter-name \"QUAL30\" \\ --filter-expression \"SOR > 3.0\" --filter-name \"SOR3\" \\ --filter-expression \"FS > 60.0\" --filter-name \"FS60\" \\ --filter-expression \"MQ < 40.0\" --filter-name \"MQ40\" \\ --filter-expression \"MQRankSum < -12.5\" --filter-name \"MQRankSum-12.5\" \\ --filter-expression \"ReadPosRankSum < -8.0\" --filter-name \"ReadPosRankSum-8\" \\ --output trio.SNP.filtered.vcf There are no differences in the number of records: grep -v \"^#\" variants/trio.SNP.vcf | wc -l and grep -v \"^#\" variants/trio.SNP.filtered.vcf | wc -l both give 446. However, there are SNPs filtered out, by changing the FILTER column. You can check the number of records with PASS by: grep -v \"^#\" variants/trio.SNP.filtered.vcf | cut -f 7 | sort | uniq -c Giving: 441 PASS 2 QD2;SOR3 3 SOR3 Filtering INDELs A command with sensible parameters to do a first iteration of hard filtering the INDELs would be: gatk VariantFiltration \\ --variant variants/trio.INDEL.vcf \\ --filter-expression \"QD < 2.0\" --filter-name \"QD2\" \\ --filter-expression \"QUAL < 30.0\" --filter-name \"QUAL30\" \\ --filter-expression \"FS > 200.0\" --filter-name \"FS200\" \\ --filter-expression \"ReadPosRankSum < -20.0\" --filter-name \"ReadPosRankSum-20\" \\ --output variants/trio.INDEL.filtered.vcf Exercise: Run the command from a script called C12_filter_INDELs.sh and figure out how many variants are filtered out. Hint You can use this command from the answer to the previous exercise: grep -v \"^#\" <variants.vcf> | cut -f 7 | sort | uniq -c to see how many INDELs were filtered out. Answer Your script: C12_filter_INDELs.sh #!/usr/bin/env bash cd ~/project/results/variants gatk VariantFiltration \\ --variant trio.INDEL.vcf \\ --filter-expression \"QD < 2.0\" --filter-name \"QD2\" \\ --filter-expression \"QUAL < 30.0\" --filter-name \"QUAL30\" \\ --filter-expression \"FS > 200.0\" --filter-name \"FS200\" \\ --filter-expression \"ReadPosRankSum < -20.0\" --filter-name \"ReadPosRankSum-20\" \\ --output trio.INDEL.filtered.vcf And check out the contents of the FILTER column: grep -v \"^#\" variants/trio.INDEL.filtered.vcf | cut -f 7 | sort | uniq -c gives: 110 PASS So no variants are filtered out. Merging filtered SNPs and INDELs Now that we have filtered the INDELs and SNPs separately, we can merge them again with this command: gatk MergeVcfs \\ --INPUT <input1.vcf> \\ --INPUT <input2.vcf> \\ --OUTPUT <merged.vcf> Exercise: Run this command from a script called C13_merge_filtered.sh to merge the vcfs ( trio.SNP.filtered.vcf and trio.INDEL.filtered.vcf ). Answer C13_merged_filtered.sh #!/usr/bin/env bash cd ~/project/results/variants gatk MergeVcfs \\ --INPUT trio.SNP.filtered.vcf \\ --INPUT trio.INDEL.filtered.vcf \\ --OUTPUT trio.filtered.vcf 2. Evaluation by concordance For this region we have a highly curated truth set for the mother available. It originates from the Illumina Platinum truth set . You can find it at data/variants/NA12878.vcf.gz To check how well we did, we\u2019d first need to extract a vcf with only the information of the mother. Exercise: Generate a script called C14_extract_mother_only.sh to extract variants that have at least one alternative allele in the mother from variants/trio.filtered.vcf . Use gatk SelectVariants with the arguments: --sample-name mother --exclude-non-variants --remove-unused-alternates In addition to the required arguments. Answer C14_extract_mother_only.sh #!/usr/bin/env bash cd ~/project/results/variants gatk SelectVariants \\ --variant trio.filtered.vcf \\ --sample-name mother \\ --exclude-non-variants \\ --remove-unused-alternates \\ --output mother.trio.filtered.vcf Exercise: A. How many variants are in the extracted vcf? How many of those are filtered out? B. Compare our vcf with the curated truth set with the command below from a script called C15_evaluate_concordance.sh . How many SNPs didn\u2019t we detect? gatk Concordance \\ --evaluation variants/mother.trio.filtered.vcf \\ --truth data/variants/NA12878.vcf.gz \\ --intervals chr20:10018000-10220000 \\ --summary variants/concordance.mother.trio.filtered Answer To get the number of records per FILTER, we run: grep -v \"^#\" variants/mother.trio.filtered.vcf | cut -f 7 | sort | uniq -c gives: 407 PASS 2 SOR3 So two records were filtered out, based on the Symmetric Odds Ratio (issues with strand bias). Your script to evaluate the concordance: C15_evaluate_concordance.sh #!/usr/bin/env bash cd ~/project gatk Concordance \\ --evaluation results/variants/mother.trio.filtered.vcf \\ --truth data/variants/NA12878.vcf.gz \\ --intervals chr20:10018000-10220000 \\ --summary results/variants/concordance.mother.trio.filtered Check out the output with cat : cat variants/concordance.mother.trio.filtered gives: type TP FP FN RECALL PRECISION SNP 319 5 9 0.973 0.985 INDEL 63 20 6 0.913 0.759 Showing that there were 9 false negatives, i.e. SNPs we didn\u2019t detect. Recall & precision More info on the definition of recall and precision on this wikipedia page Exercise: Check out the concordance of the mother with the truth set before filtering. Do this by generating two scripts: C16_extract_mother_before_filtering.sh : to run gatk SelectVariants in order to get only variants from the mother from the unfiltered trio.vcf . C17_evaluate_concordance_before_filtering.sh : to run gatk Concordance on the selected variants. Did filtering improve the recall or precision? Note We did the filtering on trio.vcf , therefore, you first have to extract the records that only apply to the mother by using gatk SelectVariants . Also note that trio.vcf contains records other than SNPs and INDELs. Use --select-type-to-include to select only SNPs and INDELs. Answer First select only SNPs and INDELs from the mother from the unfiltered vcf: C16_extract_mother_before_filtering.sh #!/usr/bin/env bash cd ~/project/results/variants gatk SelectVariants \\ --variant trio.vcf \\ --sample-name mother \\ --exclude-non-variants \\ --remove-unused-alternates \\ --select-type-to-include INDEL \\ --select-type-to-include SNP \\ --output mother.trio.vcf Get the concordance with the truth set: C17_evaluate_concordance_before_filtering.sh #!/usr/bin/env bash cd ~/project gatk Concordance \\ --evaluation results/variants/mother.trio.vcf \\ --truth data/variants/NA12878.vcf.gz \\ --intervals chr20:10018000-10220000 \\ --summary results/variants/concordance.mother.trio Which gives: type TP FP FN RECALL PRECISION SNP 319 7 9 0.973 0.979 INDEL 63 20 6 0.913 0.759 The precision for SNPs is slightly lower. Due to filtering, we removed two false positives.","title":"Filtering & evaluation"},{"location":"day2/filtering_evaluation/#learning-outcomes","text":"After having completed this chapter you will be able to: Explain why using Variant Quality Score Recalibration (VQSR) for filtering variants can outperform hard filtering Perform hard filtering on both SNPs and INDELs separately by using gatk SelectVariants in combination with gatk VariantFiltration Perform concordance between called variants and a truth set and evaluate performance of a variant calling workflow","title":"Learning outcomes"},{"location":"day2/filtering_evaluation/#material","text":"Download the presentation","title":"Material"},{"location":"day2/filtering_evaluation/#exercises","text":"","title":"Exercises"},{"location":"day2/filtering_evaluation/#1-hard-filtering","text":"The developers of gatk strongly advise to do Variant Quality Score Recalibration (VQSR) for filtering SNPs and INDELs. However, this is not always possible. For example, in the case of limited data availability and/or in the case you are working with non-model organisms and/or in the case you are a bit lazy and okay with a number of false positives. Our dataset is too small to apply VQSR. We will therefore do hard filtering instead.","title":"1. Hard filtering"},{"location":"day2/filtering_evaluation/#splitting-snps-and-indels","text":"First, filtering thresholds are usually different for SNPs and INDELs. Therefore, we will split trio.vcf into two vcfs, one containg only SNPs, and one containing only INDELs. You can extract all the SNP records in our trio vcf like this: cd ~/project/results gatk SelectVariants \\ --variant variants/trio.vcf \\ --select-type-to-include SNP \\ --output variants/trio.SNP.vcf Exercise: Check out the documentation of gatk SelectVariants , and: Figure out what you\u2019ll need to write at --select-type-to-include if you want to select only INDELS. Make a script (named C09_select_SNPs.sh ) to generate a vcf with only the SNPs Make a script (named C10_select_INDELs.sh ) to generate a second vcf with only the INDELs from trio.vcf . Answer You will need to fill in INDEL at --select-type-to-include to filter for INDELs. To get the SNPs you can run the command above: C09_select_SNPs.sh #!/usr/bin/env bash cd ~/project gatk SelectVariants \\ --variant results/variants/trio.vcf \\ --select-type-to-include SNP \\ --output results/variants/trio.SNP.vcf To get the INDELs you\u2019ll need to change --select-type-to-include to INDEL : C10_select_INDELs.sh #!/usr/bin/env bash cd ~/project gatk SelectVariants \\ --variant results/variants/trio.vcf \\ --select-type-to-include INDEL \\ --output results/variants/trio.INDEL.vcf","title":"Splitting SNPs and INDELs"},{"location":"day2/filtering_evaluation/#filtering-snps","text":"The command gatk VariantFiltration enables you to filter for both the INFO field (per variant) and FORMAT field (per genotype). For now we\u2019re only interested in filtering variants. Below you can find the command to hard-filter the SNP variants on some sensible thresholds (that are explained here ). gatk VariantFiltration \\ --variant variants/trio.SNP.vcf \\ --filter-expression \"QD < 2.0\" --filter-name \"QD2\" \\ --filter-expression \"QUAL < 30.0\" --filter-name \"QUAL30\" \\ --filter-expression \"SOR > 3.0\" --filter-name \"SOR3\" \\ --filter-expression \"FS > 60.0\" --filter-name \"FS60\" \\ --filter-expression \"MQ < 40.0\" --filter-name \"MQ40\" \\ --filter-expression \"MQRankSum < -12.5\" --filter-name \"MQRankSum-12.5\" \\ --filter-expression \"ReadPosRankSum < -8.0\" --filter-name \"ReadPosRankSum-8\" \\ --output variants/trio.SNP.filtered.vcf Exercise: Run the filtering command above in a script called C11_filter_SNPs.sh . Did it affect the number of records in the vcf? Hint You can check out the number of records in a vcf with: grep -v \"^#\" <variants.vcf> | wc -l Answer Your script: C11_filter_SNPs.sh #!/usr/bin/env bash cd ~/project/results/variants gatk VariantFiltration \\ --variant trio.SNP.vcf \\ --filter-expression \"QD < 2.0\" --filter-name \"QD2\" \\ --filter-expression \"QUAL < 30.0\" --filter-name \"QUAL30\" \\ --filter-expression \"SOR > 3.0\" --filter-name \"SOR3\" \\ --filter-expression \"FS > 60.0\" --filter-name \"FS60\" \\ --filter-expression \"MQ < 40.0\" --filter-name \"MQ40\" \\ --filter-expression \"MQRankSum < -12.5\" --filter-name \"MQRankSum-12.5\" \\ --filter-expression \"ReadPosRankSum < -8.0\" --filter-name \"ReadPosRankSum-8\" \\ --output trio.SNP.filtered.vcf There are no differences in the number of records: grep -v \"^#\" variants/trio.SNP.vcf | wc -l and grep -v \"^#\" variants/trio.SNP.filtered.vcf | wc -l both give 446. However, there are SNPs filtered out, by changing the FILTER column. You can check the number of records with PASS by: grep -v \"^#\" variants/trio.SNP.filtered.vcf | cut -f 7 | sort | uniq -c Giving: 441 PASS 2 QD2;SOR3 3 SOR3","title":"Filtering SNPs"},{"location":"day2/filtering_evaluation/#filtering-indels","text":"A command with sensible parameters to do a first iteration of hard filtering the INDELs would be: gatk VariantFiltration \\ --variant variants/trio.INDEL.vcf \\ --filter-expression \"QD < 2.0\" --filter-name \"QD2\" \\ --filter-expression \"QUAL < 30.0\" --filter-name \"QUAL30\" \\ --filter-expression \"FS > 200.0\" --filter-name \"FS200\" \\ --filter-expression \"ReadPosRankSum < -20.0\" --filter-name \"ReadPosRankSum-20\" \\ --output variants/trio.INDEL.filtered.vcf Exercise: Run the command from a script called C12_filter_INDELs.sh and figure out how many variants are filtered out. Hint You can use this command from the answer to the previous exercise: grep -v \"^#\" <variants.vcf> | cut -f 7 | sort | uniq -c to see how many INDELs were filtered out. Answer Your script: C12_filter_INDELs.sh #!/usr/bin/env bash cd ~/project/results/variants gatk VariantFiltration \\ --variant trio.INDEL.vcf \\ --filter-expression \"QD < 2.0\" --filter-name \"QD2\" \\ --filter-expression \"QUAL < 30.0\" --filter-name \"QUAL30\" \\ --filter-expression \"FS > 200.0\" --filter-name \"FS200\" \\ --filter-expression \"ReadPosRankSum < -20.0\" --filter-name \"ReadPosRankSum-20\" \\ --output trio.INDEL.filtered.vcf And check out the contents of the FILTER column: grep -v \"^#\" variants/trio.INDEL.filtered.vcf | cut -f 7 | sort | uniq -c gives: 110 PASS So no variants are filtered out.","title":"Filtering INDELs"},{"location":"day2/filtering_evaluation/#merging-filtered-snps-and-indels","text":"Now that we have filtered the INDELs and SNPs separately, we can merge them again with this command: gatk MergeVcfs \\ --INPUT <input1.vcf> \\ --INPUT <input2.vcf> \\ --OUTPUT <merged.vcf> Exercise: Run this command from a script called C13_merge_filtered.sh to merge the vcfs ( trio.SNP.filtered.vcf and trio.INDEL.filtered.vcf ). Answer C13_merged_filtered.sh #!/usr/bin/env bash cd ~/project/results/variants gatk MergeVcfs \\ --INPUT trio.SNP.filtered.vcf \\ --INPUT trio.INDEL.filtered.vcf \\ --OUTPUT trio.filtered.vcf","title":"Merging filtered SNPs and INDELs"},{"location":"day2/filtering_evaluation/#2-evaluation-by-concordance","text":"For this region we have a highly curated truth set for the mother available. It originates from the Illumina Platinum truth set . You can find it at data/variants/NA12878.vcf.gz To check how well we did, we\u2019d first need to extract a vcf with only the information of the mother. Exercise: Generate a script called C14_extract_mother_only.sh to extract variants that have at least one alternative allele in the mother from variants/trio.filtered.vcf . Use gatk SelectVariants with the arguments: --sample-name mother --exclude-non-variants --remove-unused-alternates In addition to the required arguments. Answer C14_extract_mother_only.sh #!/usr/bin/env bash cd ~/project/results/variants gatk SelectVariants \\ --variant trio.filtered.vcf \\ --sample-name mother \\ --exclude-non-variants \\ --remove-unused-alternates \\ --output mother.trio.filtered.vcf Exercise: A. How many variants are in the extracted vcf? How many of those are filtered out? B. Compare our vcf with the curated truth set with the command below from a script called C15_evaluate_concordance.sh . How many SNPs didn\u2019t we detect? gatk Concordance \\ --evaluation variants/mother.trio.filtered.vcf \\ --truth data/variants/NA12878.vcf.gz \\ --intervals chr20:10018000-10220000 \\ --summary variants/concordance.mother.trio.filtered Answer To get the number of records per FILTER, we run: grep -v \"^#\" variants/mother.trio.filtered.vcf | cut -f 7 | sort | uniq -c gives: 407 PASS 2 SOR3 So two records were filtered out, based on the Symmetric Odds Ratio (issues with strand bias). Your script to evaluate the concordance: C15_evaluate_concordance.sh #!/usr/bin/env bash cd ~/project gatk Concordance \\ --evaluation results/variants/mother.trio.filtered.vcf \\ --truth data/variants/NA12878.vcf.gz \\ --intervals chr20:10018000-10220000 \\ --summary results/variants/concordance.mother.trio.filtered Check out the output with cat : cat variants/concordance.mother.trio.filtered gives: type TP FP FN RECALL PRECISION SNP 319 5 9 0.973 0.985 INDEL 63 20 6 0.913 0.759 Showing that there were 9 false negatives, i.e. SNPs we didn\u2019t detect. Recall & precision More info on the definition of recall and precision on this wikipedia page Exercise: Check out the concordance of the mother with the truth set before filtering. Do this by generating two scripts: C16_extract_mother_before_filtering.sh : to run gatk SelectVariants in order to get only variants from the mother from the unfiltered trio.vcf . C17_evaluate_concordance_before_filtering.sh : to run gatk Concordance on the selected variants. Did filtering improve the recall or precision? Note We did the filtering on trio.vcf , therefore, you first have to extract the records that only apply to the mother by using gatk SelectVariants . Also note that trio.vcf contains records other than SNPs and INDELs. Use --select-type-to-include to select only SNPs and INDELs. Answer First select only SNPs and INDELs from the mother from the unfiltered vcf: C16_extract_mother_before_filtering.sh #!/usr/bin/env bash cd ~/project/results/variants gatk SelectVariants \\ --variant trio.vcf \\ --sample-name mother \\ --exclude-non-variants \\ --remove-unused-alternates \\ --select-type-to-include INDEL \\ --select-type-to-include SNP \\ --output mother.trio.vcf Get the concordance with the truth set: C17_evaluate_concordance_before_filtering.sh #!/usr/bin/env bash cd ~/project gatk Concordance \\ --evaluation results/variants/mother.trio.vcf \\ --truth data/variants/NA12878.vcf.gz \\ --intervals chr20:10018000-10220000 \\ --summary results/variants/concordance.mother.trio Which gives: type TP FP FN RECALL PRECISION SNP 319 7 9 0.973 0.979 INDEL 63 20 6 0.913 0.759 The precision for SNPs is slightly lower. Due to filtering, we removed two false positives.","title":"2. Evaluation by concordance"},{"location":"day2/variant_calling/","text":"Learning outcomes After having completed this chapter you will be able to: Perform basic calculations regarding the genotype likelihood of individual variants Follow gatk best practices workflow to perform a variant analysis by: Calling variants with gatk HaplotypeCaller Combining multiple vcf files into a single vcf file Perform basic operations to get statistics of a vcf file Material The paper on genomic variant call format (gVCF) GATK best practices germline short variant workflow : Exercises 1. Variant calling Calculating PL and GQ by hand Here\u2019s a function in R to calculate genotype likelihoods as described in Li H. Bioinformatics. 2011;27:2987\u201393 (assuming equal base error probabilities for all reads): genotype_likelihood <- function ( m , g , e , ref , alt ){ ((( m - g ) * e + g * ( 1 - e )) ^ alt * (( m - g ) * ( 1 - e ) + g * e ) ^ ref ) / ( m ^ ( ref + alt )) } Where: m : ploidy g : number of alternative alleles e : base error probability ref : number of reference alleles counted alt : number of alternative alleles counted Exercise: In the scripts directory, create a script called calculate_genotype_likelihoods.R . Copy-paste the above function to the script, and use it to calculate the three genotype likelihoods (for g = 0, g = 1 and g = 2) for a case where we count 22 reference alleles and 4 alternative alleles (so a coverage of 26), and base error probability of 0.01. Calculate the PL values ( -10*log10(likelihood) ) for each genotype. Using VScode with R In order to easily interact with your R script, you can do the following: Open the R script in VS code In the terminal, type R to start the R console Select the code you\u2019d like to run in the R script Type Ctrl + Enter to send it to the console After you have finished, type quit() in the R console. Answer # For g = 0 (i.e. 0 alternative alleles) -10 * log10 ( genotype_likelihood ( m = 2 , g = 0 , e = 0.01 , ref = 22 , alt = 4 )) # [1] 80.96026 -10 * log10 ( genotype_likelihood ( m = 2 , g = 1 , e = 0.01 , ref = 22 , alt = 4 )) # [1] 78.2678 -10 * log10 ( genotype_likelihood ( m = 2 , g = 2 , e = 0.01 , ref = 22 , alt = 4 )) # [1] 440.1746 Exercise: What is the most likely genotype? What is the genotype quality (GQ)? Do you think we should be confident about this genotype call? Answer The most likely genotype has the lowest PL, so where g=1 (heterozygous). GQ is calculated by subtracting the lowest PL from the second lowest PL, so 80.96 - 78.27 = 2.69. This is a low genotype quality (note that we\u2019re in the phred scale), i.e. an error probability of 0.54. This makes sense, if the genotype is heterozygous we would roughly expect to count as many reference as alternative alleles, and our example quite strongly deviates from this expectation. Calling variants with GATK The command gatk HaplotypeCaller is the core command of gatk . It performs the actual variant calling. Exercise: Check out the gatk HaplotypeCaller documentation , and find out which arguments are required. Answer Required arguments are: --input --ouput --reference Exercise: Generate a script called B10_run_haplotype_caller.sh in B-mother_only . Use it to make a directory called ~/project/results/variants to write the output vcf. In the same script, run gatk HaplotypeCaller with required options on the recalibrated alignment file of the mother ( results/bqsr/mother.recal.bam ). We\u2019ll focus on a small region, so add --intervals chr20:10018000-10220000 . Answer B10_run_haplotype_caller.sh #!/usr/bin/env bash cd ~/project mkdir -p results/variants gatk HaplotypeCaller \\ --reference data/reference/Homo_sapiens.GRCh38.dna.chromosome.20.fa \\ --input results/bqsr/mother.recal.bam \\ --output results/variants/mother.HC.vcf \\ --intervals chr20:10018000-10220000 Exercise: You can get the number of records in a vcf with piping the output of grep -v '^#' to wc -l . Get the number of variants in the vcf. Answer grep -v '^#' variants/mother.HC.vcf | wc -l Shows you that there are 411 variants in there. You can get some more statistics with gatk VariantsToTable . The output can be used to easily query things in R or MS Excel. Here\u2019s an example: gatk VariantsToTable \\ --variant variants/mother.HC.vcf \\ --fields CHROM -F POS -F TYPE -GF GT \\ --output variants/mother.HC.table Exercise: Run the command from within a script called B11_variants_to_table.sh , and have a look at the first few records (use e.g. head or less ). After that, report the number of SNPs and INDELs. Answer Your script should look like: B11_variants_to_table.sh cd ~/project gatk VariantsToTable \\ --variant results/variants/mother.HC.vcf \\ --fields CHROM -F POS -F TYPE -GF GT \\ --output results/variants/mother.HC.table You can get the number of SNPs with: grep -c \"SNP\" variants/mother.HC.table which will give 326 And the number of INDELs with: grep -c \"INDEL\" variants/mother.HC.table that outputs 84 A more fancy way to this would be: cut -f 3 variants/mother.HC.table | tail -n +2 | sort | uniq -c Giving: 84 INDEL 1 MIXED 326 SNP Now, we will perform the variant calling on all three samples. Later we want to combine the variant calls. For efficient merging of vcfs, we will need to output the variants as a GVCF. To do that, we will use the option --emit-ref-confidence GVCF . Also, we\u2019ll visualise the haplotype phasing with IGV in the next section. For that we\u2019ll need a phased bam. You can get this output with the argument --bam-output . Exercise: Create a script in C-all_samples called C06_run_haplotypecaller.sh . Use it to run gatk HaplotypeCaller for mother, father and son in a loop. Use the same arguments as in the previous exercise. On top of that, add the arguments --emit-ref-confidence GVCF and --bamoutput <phased.bam> . Answer C06_run_haplotypecaller.sh #!/usr/bin/env bash cd ~/project for SAMPLE in mother father son do gatk HaplotypeCaller \\ --reference data/reference/Homo_sapiens.GRCh38.dna.chromosome.20.fa \\ --input results/bqsr/ \" $SAMPLE \" .recal.bam \\ --output results/variants/ \" $SAMPLE \" .HC.g.vcf \\ --bam-output results/variants/ \" $SAMPLE \" .phased.bam \\ --intervals chr20:10018000-10220000 \\ --emit-ref-confidence GVCF done 2. Combining GVCFs Now that we have all three GVCFs of the mother, father and son, we can combine them into a database. We do this because it enables us to later add GVCFs (with the option --genomicsdb-update-workspace-path ), and to efficiently combine them into a single vcf. You can generate a GenomicsDB on our three samples like this: C07_create_genomicsdb.sh #!/usr/bin/env bash cd ~/project gatk GenomicsDBImport \\ --variant results/variants/mother.HC.g.vcf \\ --variant results/variants/father.HC.g.vcf \\ --variant results/variants/son.HC.g.vcf \\ --intervals chr20:10018000-10220000 \\ --genomicsdb-workspace-path results/genomicsdb Exercise: Create a script called C07_create_genomicsdb.sh to run this command to generate the database. You can retrieve the combined vcf from the database with gatk GenotypeGVCFs . C08_genotype_gvcfs.sh #!/usr/bin/env bash cd ~/project gatk GenotypeGVCFs \\ --reference data/reference/Homo_sapiens.GRCh38.dna.chromosome.20.fa \\ --variant gendb://results/genomicsdb \\ --intervals chr20:10018000-10220000 \\ --output results/variants/trio.vcf Exercise: Create a script called C08_genotype_gvcfs.sh to run this command to generate the combined vcf.","title":"Variant calling"},{"location":"day2/variant_calling/#learning-outcomes","text":"After having completed this chapter you will be able to: Perform basic calculations regarding the genotype likelihood of individual variants Follow gatk best practices workflow to perform a variant analysis by: Calling variants with gatk HaplotypeCaller Combining multiple vcf files into a single vcf file Perform basic operations to get statistics of a vcf file","title":"Learning outcomes"},{"location":"day2/variant_calling/#material","text":"The paper on genomic variant call format (gVCF) GATK best practices germline short variant workflow :","title":"Material"},{"location":"day2/variant_calling/#exercises","text":"","title":"Exercises"},{"location":"day2/variant_calling/#1-variant-calling","text":"","title":"1. Variant calling"},{"location":"day2/variant_calling/#calculating-pl-and-gq-by-hand","text":"Here\u2019s a function in R to calculate genotype likelihoods as described in Li H. Bioinformatics. 2011;27:2987\u201393 (assuming equal base error probabilities for all reads): genotype_likelihood <- function ( m , g , e , ref , alt ){ ((( m - g ) * e + g * ( 1 - e )) ^ alt * (( m - g ) * ( 1 - e ) + g * e ) ^ ref ) / ( m ^ ( ref + alt )) } Where: m : ploidy g : number of alternative alleles e : base error probability ref : number of reference alleles counted alt : number of alternative alleles counted Exercise: In the scripts directory, create a script called calculate_genotype_likelihoods.R . Copy-paste the above function to the script, and use it to calculate the three genotype likelihoods (for g = 0, g = 1 and g = 2) for a case where we count 22 reference alleles and 4 alternative alleles (so a coverage of 26), and base error probability of 0.01. Calculate the PL values ( -10*log10(likelihood) ) for each genotype. Using VScode with R In order to easily interact with your R script, you can do the following: Open the R script in VS code In the terminal, type R to start the R console Select the code you\u2019d like to run in the R script Type Ctrl + Enter to send it to the console After you have finished, type quit() in the R console. Answer # For g = 0 (i.e. 0 alternative alleles) -10 * log10 ( genotype_likelihood ( m = 2 , g = 0 , e = 0.01 , ref = 22 , alt = 4 )) # [1] 80.96026 -10 * log10 ( genotype_likelihood ( m = 2 , g = 1 , e = 0.01 , ref = 22 , alt = 4 )) # [1] 78.2678 -10 * log10 ( genotype_likelihood ( m = 2 , g = 2 , e = 0.01 , ref = 22 , alt = 4 )) # [1] 440.1746 Exercise: What is the most likely genotype? What is the genotype quality (GQ)? Do you think we should be confident about this genotype call? Answer The most likely genotype has the lowest PL, so where g=1 (heterozygous). GQ is calculated by subtracting the lowest PL from the second lowest PL, so 80.96 - 78.27 = 2.69. This is a low genotype quality (note that we\u2019re in the phred scale), i.e. an error probability of 0.54. This makes sense, if the genotype is heterozygous we would roughly expect to count as many reference as alternative alleles, and our example quite strongly deviates from this expectation.","title":"Calculating PL and GQ by hand"},{"location":"day2/variant_calling/#calling-variants-with-gatk","text":"The command gatk HaplotypeCaller is the core command of gatk . It performs the actual variant calling. Exercise: Check out the gatk HaplotypeCaller documentation , and find out which arguments are required. Answer Required arguments are: --input --ouput --reference Exercise: Generate a script called B10_run_haplotype_caller.sh in B-mother_only . Use it to make a directory called ~/project/results/variants to write the output vcf. In the same script, run gatk HaplotypeCaller with required options on the recalibrated alignment file of the mother ( results/bqsr/mother.recal.bam ). We\u2019ll focus on a small region, so add --intervals chr20:10018000-10220000 . Answer B10_run_haplotype_caller.sh #!/usr/bin/env bash cd ~/project mkdir -p results/variants gatk HaplotypeCaller \\ --reference data/reference/Homo_sapiens.GRCh38.dna.chromosome.20.fa \\ --input results/bqsr/mother.recal.bam \\ --output results/variants/mother.HC.vcf \\ --intervals chr20:10018000-10220000 Exercise: You can get the number of records in a vcf with piping the output of grep -v '^#' to wc -l . Get the number of variants in the vcf. Answer grep -v '^#' variants/mother.HC.vcf | wc -l Shows you that there are 411 variants in there. You can get some more statistics with gatk VariantsToTable . The output can be used to easily query things in R or MS Excel. Here\u2019s an example: gatk VariantsToTable \\ --variant variants/mother.HC.vcf \\ --fields CHROM -F POS -F TYPE -GF GT \\ --output variants/mother.HC.table Exercise: Run the command from within a script called B11_variants_to_table.sh , and have a look at the first few records (use e.g. head or less ). After that, report the number of SNPs and INDELs. Answer Your script should look like: B11_variants_to_table.sh cd ~/project gatk VariantsToTable \\ --variant results/variants/mother.HC.vcf \\ --fields CHROM -F POS -F TYPE -GF GT \\ --output results/variants/mother.HC.table You can get the number of SNPs with: grep -c \"SNP\" variants/mother.HC.table which will give 326 And the number of INDELs with: grep -c \"INDEL\" variants/mother.HC.table that outputs 84 A more fancy way to this would be: cut -f 3 variants/mother.HC.table | tail -n +2 | sort | uniq -c Giving: 84 INDEL 1 MIXED 326 SNP Now, we will perform the variant calling on all three samples. Later we want to combine the variant calls. For efficient merging of vcfs, we will need to output the variants as a GVCF. To do that, we will use the option --emit-ref-confidence GVCF . Also, we\u2019ll visualise the haplotype phasing with IGV in the next section. For that we\u2019ll need a phased bam. You can get this output with the argument --bam-output . Exercise: Create a script in C-all_samples called C06_run_haplotypecaller.sh . Use it to run gatk HaplotypeCaller for mother, father and son in a loop. Use the same arguments as in the previous exercise. On top of that, add the arguments --emit-ref-confidence GVCF and --bamoutput <phased.bam> . Answer C06_run_haplotypecaller.sh #!/usr/bin/env bash cd ~/project for SAMPLE in mother father son do gatk HaplotypeCaller \\ --reference data/reference/Homo_sapiens.GRCh38.dna.chromosome.20.fa \\ --input results/bqsr/ \" $SAMPLE \" .recal.bam \\ --output results/variants/ \" $SAMPLE \" .HC.g.vcf \\ --bam-output results/variants/ \" $SAMPLE \" .phased.bam \\ --intervals chr20:10018000-10220000 \\ --emit-ref-confidence GVCF done","title":"Calling variants with GATK"},{"location":"day2/variant_calling/#2-combining-gvcfs","text":"Now that we have all three GVCFs of the mother, father and son, we can combine them into a database. We do this because it enables us to later add GVCFs (with the option --genomicsdb-update-workspace-path ), and to efficiently combine them into a single vcf. You can generate a GenomicsDB on our three samples like this: C07_create_genomicsdb.sh #!/usr/bin/env bash cd ~/project gatk GenomicsDBImport \\ --variant results/variants/mother.HC.g.vcf \\ --variant results/variants/father.HC.g.vcf \\ --variant results/variants/son.HC.g.vcf \\ --intervals chr20:10018000-10220000 \\ --genomicsdb-workspace-path results/genomicsdb Exercise: Create a script called C07_create_genomicsdb.sh to run this command to generate the database. You can retrieve the combined vcf from the database with gatk GenotypeGVCFs . C08_genotype_gvcfs.sh #!/usr/bin/env bash cd ~/project gatk GenotypeGVCFs \\ --reference data/reference/Homo_sapiens.GRCh38.dna.chromosome.20.fa \\ --variant gendb://results/genomicsdb \\ --intervals chr20:10018000-10220000 \\ --output results/variants/trio.vcf Exercise: Create a script called C08_genotype_gvcfs.sh to run this command to generate the combined vcf.","title":"2. Combining GVCFs"},{"location":"day2/variant_calling_preparation/","text":"Learning outcomes After having completed this chapter you will be able to: Describe how variant information is stored in a variant call format ( .vcf ) file Describe the \u2018missing genotype problem\u2019 when calling variants of multiple samples, and the different methods on how this can be solved Applying Base Quality Score Recalibration on an alignment file Material Download the presentation VCF format description GATK best practices germline short variant workflow : Exercises 1. Indexing, indexing, indexing Many algorithms work faster, or only work with an index of their (large) input files. In that sense, gatk is no different from other tools. The index for a reference needs to be created in two steps: cd ~/project/data/reference samtools faidx <reference.fa> gatk CreateSequenceDictionary --REFERENCE <reference.fa> Also input vcf files need to be indexed. This will create a .idx file associated with the .vcf . You can do this like this: gatk IndexFeatureFile --input <variants.vcf> Exercise: Create two scripts in A-prepare_references to generate the required indexes: A03_create_vcf_indices.sh , in which you create indices for: A part of the dbsnp database: variants/GCF.38.filtered.renamed.vcf A part of the 1000 genomes indel golden standard: variants/1000g_gold_standard.indels.filtered.vcf A04_create_fasta_index.sh , in which you create an index for: The reference genome reference/Homo_sapiens.GRCh38.dna.chromosome.20.fa Note Indexes are often stored in the same directory as the indexed file. For the vcf and fasta indexes this is also the case. Answer Creating the indices for the vcfs: A03_create_vcf_indices.sh #!/usr/bin/env bash cd ~/project/data gatk IndexFeatureFile --input variants/1000g_gold_standard.indels.filtered.vcf gatk IndexFeatureFile --input variants/GCF.38.filtered.renamed.vcf Creating the index for the reference genome: A04_create_fasta_index.sh #!/usr/bin/env bash cd ~/project/data samtools faidx reference/Homo_sapiens.GRCh38.dna.chromosome.20.fa gatk CreateSequenceDictionary --REFERENCE reference/Homo_sapiens.GRCh38.dna.chromosome.20.fa Chromosome names Unlike IGV, gatk requires equal chromosome names for all its input files and indexes, e.g. in .fasta , .bam and .vcf files. In general, for the human genome there are three types of chromosome names: Just a number, e.g. 20 Prefixed by chr . e.g. chr20 Refseq name, e.g. NC_000020.11 Before you start the alignment, it\u2019s wise to check out what chromosome naming your input files are using, because changing chromosome names in a .fasta file is easier than in a .bam file. If your fasta titles are e.g. starting with a number you can add chr to it with sed : sed 's/^>/>chr/g' <reference.fasta> You can change chromsome names in a vcf with bcftools annotate : bcftools annotate --rename-chrs <tab-delimited-renaming> <input.vcf> 2. Base Quality Score Recalibration (BQSR) BQSR evaluates the base qualities on systematic error. It can ignore sites with known variants. BQSR helps to identify faulty base calls, and therefore reduces the chance on discovering false positive variant positions. BQSR is done in two steps: Recalibration with gatk BaseRecalibrator By using the output of gatk BaseRecalibrator , the application to the bam file with gatk ApplyBQSR Exercise: Check out the documentation of the tools. Which options are required? Answer For gatk BaseRecalibrator : --reference --input --known-sites --output For gatk ApplyBQSR : --bqsr-recal-file --input --output Exercise: Create a script in B-mother_only called B09_perform_bqsr.sh to execute the two bqsr commands. Do this with the required options on mother.rg.md.bam . At --known-sites specify variants/1000g_gold_standard.indels.filtered.vcf and variants/GCF.38.filtered.renamed.vcf . Multiple inputs for same argument In some cases you need to add multiple inputs (e.g. multiple vcf files) into the same argument (e.g. --known-sites ). To provide multiple inputs for the same argument in gatk , you can use the same argument multiple times, e.g.: gatk BaseRecalibrator \\ --reference <reference.fa> \\ --input <alignment.bam> \\ --known-sites <variants1.vcf> \\ --known-sites <variants2.vcf> \\ --output <output.table> Answer B09_perform_bqsr.sh #!/usr/bin/env bash cd ~/project mkdir -p results/bqsr gatk BaseRecalibrator \\ --reference data/reference/Homo_sapiens.GRCh38.dna.chromosome.20.fa \\ --input results/alignments/mother.rg.md.bam \\ --known-sites data/variants/GCF.38.filtered.renamed.vcf \\ --known-sites data/variants/1000g_gold_standard.indels.filtered.vcf \\ --output results/bqsr/mother.recal.table gatk ApplyBQSR \\ --input results/alignments/mother.rg.md.bam \\ --bqsr-recal-file results/bqsr/mother.recal.table \\ --output results/bqsr/mother.recal.bam Exercise: Place these commands in a \u2018for loop\u2019, that performs the BQSR for mother, father and son. Do this with a script called C05_perform_bqsr.sh in C-all_samples . Answer C05_perform_bqsr.sh #!/usr/bin/env bash cd ~/project for SAMPLE in mother father son do gatk BaseRecalibrator \\ --reference data/reference/Homo_sapiens.GRCh38.dna.chromosome.20.fa \\ --input results/alignments/ \" $SAMPLE \" .rg.md.bam \\ --known-sites data/variants/GCF.38.filtered.renamed.vcf \\ --known-sites data/variants/1000g_gold_standard.indels.filtered.vcf \\ --output results/bqsr/ \" $SAMPLE \" .recal.table gatk ApplyBQSR \\ --input results/alignments/ \" $SAMPLE \" .rg.md.bam \\ --bqsr-recal-file results/bqsr/ \" $SAMPLE \" .recal.table \\ --output results/bqsr/ \" $SAMPLE \" .recal.bam done","title":"Variant calling - preparation"},{"location":"day2/variant_calling_preparation/#learning-outcomes","text":"After having completed this chapter you will be able to: Describe how variant information is stored in a variant call format ( .vcf ) file Describe the \u2018missing genotype problem\u2019 when calling variants of multiple samples, and the different methods on how this can be solved Applying Base Quality Score Recalibration on an alignment file","title":"Learning outcomes"},{"location":"day2/variant_calling_preparation/#material","text":"Download the presentation VCF format description GATK best practices germline short variant workflow :","title":"Material"},{"location":"day2/variant_calling_preparation/#exercises","text":"","title":"Exercises"},{"location":"day2/variant_calling_preparation/#1-indexing-indexing-indexing","text":"Many algorithms work faster, or only work with an index of their (large) input files. In that sense, gatk is no different from other tools. The index for a reference needs to be created in two steps: cd ~/project/data/reference samtools faidx <reference.fa> gatk CreateSequenceDictionary --REFERENCE <reference.fa> Also input vcf files need to be indexed. This will create a .idx file associated with the .vcf . You can do this like this: gatk IndexFeatureFile --input <variants.vcf> Exercise: Create two scripts in A-prepare_references to generate the required indexes: A03_create_vcf_indices.sh , in which you create indices for: A part of the dbsnp database: variants/GCF.38.filtered.renamed.vcf A part of the 1000 genomes indel golden standard: variants/1000g_gold_standard.indels.filtered.vcf A04_create_fasta_index.sh , in which you create an index for: The reference genome reference/Homo_sapiens.GRCh38.dna.chromosome.20.fa Note Indexes are often stored in the same directory as the indexed file. For the vcf and fasta indexes this is also the case. Answer Creating the indices for the vcfs: A03_create_vcf_indices.sh #!/usr/bin/env bash cd ~/project/data gatk IndexFeatureFile --input variants/1000g_gold_standard.indels.filtered.vcf gatk IndexFeatureFile --input variants/GCF.38.filtered.renamed.vcf Creating the index for the reference genome: A04_create_fasta_index.sh #!/usr/bin/env bash cd ~/project/data samtools faidx reference/Homo_sapiens.GRCh38.dna.chromosome.20.fa gatk CreateSequenceDictionary --REFERENCE reference/Homo_sapiens.GRCh38.dna.chromosome.20.fa Chromosome names Unlike IGV, gatk requires equal chromosome names for all its input files and indexes, e.g. in .fasta , .bam and .vcf files. In general, for the human genome there are three types of chromosome names: Just a number, e.g. 20 Prefixed by chr . e.g. chr20 Refseq name, e.g. NC_000020.11 Before you start the alignment, it\u2019s wise to check out what chromosome naming your input files are using, because changing chromosome names in a .fasta file is easier than in a .bam file. If your fasta titles are e.g. starting with a number you can add chr to it with sed : sed 's/^>/>chr/g' <reference.fasta> You can change chromsome names in a vcf with bcftools annotate : bcftools annotate --rename-chrs <tab-delimited-renaming> <input.vcf>","title":"1. Indexing, indexing, indexing"},{"location":"day2/variant_calling_preparation/#2-base-quality-score-recalibration-bqsr","text":"BQSR evaluates the base qualities on systematic error. It can ignore sites with known variants. BQSR helps to identify faulty base calls, and therefore reduces the chance on discovering false positive variant positions. BQSR is done in two steps: Recalibration with gatk BaseRecalibrator By using the output of gatk BaseRecalibrator , the application to the bam file with gatk ApplyBQSR Exercise: Check out the documentation of the tools. Which options are required? Answer For gatk BaseRecalibrator : --reference --input --known-sites --output For gatk ApplyBQSR : --bqsr-recal-file --input --output Exercise: Create a script in B-mother_only called B09_perform_bqsr.sh to execute the two bqsr commands. Do this with the required options on mother.rg.md.bam . At --known-sites specify variants/1000g_gold_standard.indels.filtered.vcf and variants/GCF.38.filtered.renamed.vcf . Multiple inputs for same argument In some cases you need to add multiple inputs (e.g. multiple vcf files) into the same argument (e.g. --known-sites ). To provide multiple inputs for the same argument in gatk , you can use the same argument multiple times, e.g.: gatk BaseRecalibrator \\ --reference <reference.fa> \\ --input <alignment.bam> \\ --known-sites <variants1.vcf> \\ --known-sites <variants2.vcf> \\ --output <output.table> Answer B09_perform_bqsr.sh #!/usr/bin/env bash cd ~/project mkdir -p results/bqsr gatk BaseRecalibrator \\ --reference data/reference/Homo_sapiens.GRCh38.dna.chromosome.20.fa \\ --input results/alignments/mother.rg.md.bam \\ --known-sites data/variants/GCF.38.filtered.renamed.vcf \\ --known-sites data/variants/1000g_gold_standard.indels.filtered.vcf \\ --output results/bqsr/mother.recal.table gatk ApplyBQSR \\ --input results/alignments/mother.rg.md.bam \\ --bqsr-recal-file results/bqsr/mother.recal.table \\ --output results/bqsr/mother.recal.bam Exercise: Place these commands in a \u2018for loop\u2019, that performs the BQSR for mother, father and son. Do this with a script called C05_perform_bqsr.sh in C-all_samples . Answer C05_perform_bqsr.sh #!/usr/bin/env bash cd ~/project for SAMPLE in mother father son do gatk BaseRecalibrator \\ --reference data/reference/Homo_sapiens.GRCh38.dna.chromosome.20.fa \\ --input results/alignments/ \" $SAMPLE \" .rg.md.bam \\ --known-sites data/variants/GCF.38.filtered.renamed.vcf \\ --known-sites data/variants/1000g_gold_standard.indels.filtered.vcf \\ --output results/bqsr/ \" $SAMPLE \" .recal.table gatk ApplyBQSR \\ --input results/alignments/ \" $SAMPLE \" .rg.md.bam \\ --bqsr-recal-file results/bqsr/ \" $SAMPLE \" .recal.table \\ --output results/bqsr/ \" $SAMPLE \" .recal.bam done","title":"2. Base Quality Score Recalibration (BQSR)"},{"location":"day2/visualisation/","text":"Learning outcomes After having completed this chapter you will be able to: Use IGV to: Visualise read alignments that support called variants Visualise phasing information generated by gatk Material IGV documentation Exercises Download the following files to your local computer: variants/mother.phased.bam variants/mother.phased.bai bqsr/mother.recal.bam bqsr/mother.recal.bai variants/mother.HC.vcf Downloading files You can download files by right-click the file and after that select Download : Launch IGV and select the human genome version hg38 as a reference. Load the downloaded files as tracks in igv with File > Load From File\u2026 , and navigate to region chr20:10,026,397-10,026,638 . Exercise: Zoom out for a bit. Not all reads are in the track of mother.phased.bam . What kind of reads are in there? Answer The reads supporting called variants. Now, we\u2019ll investigate the haplotype phasing. Go back to chr20:10,026,397-10,026,638 . Tip If your screen isn\u2019t huge, you can remove the track mother.recal.bam . Do that by right-click on the track, and click on Remove Track . In the track with mother.phased.bam , right click on the reads and select Group alignments by > read group . This splits your track in two parts, one with artificial reads describing haplotypes that were taken in consideration (ArtificalHaplotypeRG), and one with original reads that support the variants. Exercise : How many haplotypes were taken into consideration? How many haplotypes can you expect at maximum within a single individual? Hint This might come as a shock, but humans are diploid. Answer Three haplotypes, as there are three artificial reads: Diploids can carry two haplotypes. So at least one of the three is wrong. Now colour the reads by phase. Do that with by right clicking on the track and choose Colour alignments by > tag , and type in \u201cHC\u201d (that\u2019s the tag where the phasing is stored). Exercise: Which artificial read doesn\u2019t get support from the original sequence reads? Are the alternative alleles of the two SNPs on the same haplotype (i.e. in phase)? Answer The track should look like this (colours can be different): The reads only support the brown and blue haplotype, and not the pink one. The alternative alleles are coloured in IGV. For the first SNP this is the C (in blue) and for the second the T (in red). They are always in different reads, so they are in repulsion (not in phase).","title":"Visualisation"},{"location":"day2/visualisation/#learning-outcomes","text":"After having completed this chapter you will be able to: Use IGV to: Visualise read alignments that support called variants Visualise phasing information generated by gatk","title":"Learning outcomes"},{"location":"day2/visualisation/#material","text":"IGV documentation","title":"Material"},{"location":"day2/visualisation/#exercises","text":"Download the following files to your local computer: variants/mother.phased.bam variants/mother.phased.bai bqsr/mother.recal.bam bqsr/mother.recal.bai variants/mother.HC.vcf Downloading files You can download files by right-click the file and after that select Download : Launch IGV and select the human genome version hg38 as a reference. Load the downloaded files as tracks in igv with File > Load From File\u2026 , and navigate to region chr20:10,026,397-10,026,638 . Exercise: Zoom out for a bit. Not all reads are in the track of mother.phased.bam . What kind of reads are in there? Answer The reads supporting called variants. Now, we\u2019ll investigate the haplotype phasing. Go back to chr20:10,026,397-10,026,638 . Tip If your screen isn\u2019t huge, you can remove the track mother.recal.bam . Do that by right-click on the track, and click on Remove Track . In the track with mother.phased.bam , right click on the reads and select Group alignments by > read group . This splits your track in two parts, one with artificial reads describing haplotypes that were taken in consideration (ArtificalHaplotypeRG), and one with original reads that support the variants. Exercise : How many haplotypes were taken into consideration? How many haplotypes can you expect at maximum within a single individual? Hint This might come as a shock, but humans are diploid. Answer Three haplotypes, as there are three artificial reads: Diploids can carry two haplotypes. So at least one of the three is wrong. Now colour the reads by phase. Do that with by right clicking on the track and choose Colour alignments by > tag , and type in \u201cHC\u201d (that\u2019s the tag where the phasing is stored). Exercise: Which artificial read doesn\u2019t get support from the original sequence reads? Are the alternative alleles of the two SNPs on the same haplotype (i.e. in phase)? Answer The track should look like this (colours can be different): The reads only support the brown and blue haplotype, and not the pink one. The alternative alleles are coloured in IGV. For the first SNP this is the C (in blue) and for the second the T (in red). They are always in different reads, so they are in repulsion (not in phase).","title":"Exercises"}]}